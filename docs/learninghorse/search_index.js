var documenterSearchIndex = {"docs":
[{"location":"LossFunction/#LossFunction","page":"LossFunction","title":"LossFunction","text":"","category":"section"},{"location":"LossFunction/#How-to-use-MSE(Mean-Square-Error)","page":"LossFunction","title":"How to use MSE(Mean Square Error)","text":"","category":"section"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"Mean Square Error can be used as follows.","category":"page"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"using LearningHorse\nmean = Loss_Function.MSE([1, 2, 3, 4], [1, 2, 3, 4], [1, 2])\nprintln(mean) #4\n\nx = [15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\n\n#Mean square error of Gaussian basis function\nw, m, s = LinearRegression.BFM.fit(x[1, :], t, 4)\nmse = LossFunction.MSE(x[1, :], t, w, b = \"gauss\", m = m, s = s)\nprintln(\"mse(gauss):\", mse)\n#mse(gauss):16.777305554499097\n\n#Mean square error of polynomial regression\nw, m, s = LinearRegression.BFM.fit(x[1, :], t, 4, alpha = 0.1, f = \"polynomial\")\nmse = LossFunction.MSE(x[1, :], t, w, b = \"polynomial\", m = m, s = s)\nprintln(\"mse(polynomial):\", mse)\n#mse(polynomial):18.491924125591847","category":"page"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"By specifying the argument b, the MSE of the basis function model can be calculated. The arguments are as follows.","category":"page"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"Loss_Function.MSE()","category":"page"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"- x : Dependent variable\n- t : Objective variable\n- w : Array of obtained by fitting","category":"page"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"The following are keyword arguments \t- b : Specify when calculating the MSE of the basis function. \"Gauss\" for Gaussian basis functions, \"polynomial\" for polynomial regression \t- m : It can be specified when calculating the MSE of a Gaussian basis function. By default, it is calculated using the vector at the center of each Gaussian function. \t- s : It can be specified when calculating the MSE of a Gaussian function. Calculated using the interval of each Gaussian function by default \t- mean_f : Whether to divide the sum of the squared and added errors to get the mean squared error, the default value is true","category":"page"},{"location":"LossFunction/#How-to-use-CEE(Cross-Entropy-Error)","page":"LossFunction","title":"How to use CEE(Cross Entropy Error)","text":"","category":"section"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"The cross entropy error is used as follows.","category":"page"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"using LearningHorse\n#CEE of MS function\nx = [5.1 3.5; 4.9 3.0; 4.7 3.2; 4.6 3.1; 5.0 3.6; 5.4 3.9; 4.6 3.4; 5.0 3.4; 4.4 2.9; 4.9 3.1; 5.4 3.7; 4.8 3.4; 4.8 3.0; 4.3 3.0; 5.8 4.0; 5.7 4.4; 5.4 3.9; 5.1 3.5; 5.7 3.8; 5.1 3.8; 5.4 3.4; 5.1 3.7; 4.6 3.6; 5.1 3.3; 4.8 3.4; 5.0 3.0; 5.0 3.4; 5.2 3.5; 5.2 3.4; 4.7 3.2; 4.8 3.1; 5.4 3.4; 5.2 4.1; 5.5 4.2; 4.9 3.1; 5.0 3.2; 5.5 3.5; 4.9 3.6; 4.4 3.0; 5.1 3.4; 5.0 3.5; 4.5 2.3; 4.4 3.2; 5.0 3.5; 5.1 3.8; 4.8 3.0; 5.1 3.8; 4.6 3.2; 5.3 3.7; 5.0 3.3; 7.0 3.2; 6.4 3.2; 6.9 3.1; 5.5 2.3; 6.5 2.8; 5.7 2.8; 6.3 3.3; 4.9 2.4; 6.6 2.9; 5.2 2.7; 5.0 2.0; 5.9 3.0; 6.0 2.2; 6.1 2.9; 5.6 2.9; 6.7 3.1; 5.6 3.0; 5.8 2.7; 6.2 2.2; 5.6 2.5; 5.9 3.2; 6.1 2.8; 6.3 2.5; 6.1 2.8; 6.4 2.9; 6.6 3.0; 6.8 2.8; 6.7 3.0; 6.0 2.9; 5.7 2.6; 5.5 2.4; 5.5 2.4; 5.8 2.7; 6.0 2.7; 5.4 3.0; 6.0 3.4; 6.7 3.1; 6.3 2.3; 5.6 3.0; 5.5 2.5; 5.5 2.6; 6.1 3.0; 5.8 2.6; 5.0 2.3; 5.6 2.7; 5.7 3.0; 5.7 2.9; 6.2 2.9; 5.1 2.5; 5.7 2.8; 6.3 3.3; 5.8 2.7; 7.1 3.0; 6.3 2.9; 6.5 3.0; 7.6 3.0; 4.9 2.5; 7.3 2.9; 6.7 2.5; 7.2 3.6; 6.5 3.2; 6.4 2.7; 6.8 3.0; 5.7 2.5; 5.8 2.8; 6.4 3.2; 6.5 3.0; 7.7 3.8; 7.7 2.6; 6.0 2.2; 6.9 3.2; 5.6 2.8; 7.7 2.8; 6.3 2.7; 6.7 3.3; 7.2 3.2; 6.2 2.8; 6.1 3.0; 6.4 2.8; 7.2 3.0; 7.4 2.8; 7.9 3.8; 6.4 2.8; 6.3 2.8; 6.1 2.6; 7.7 3.0; 6.3 3.4; 6.4 3.1; 6.0 3.0; 6.9 3.1; 6.7 3.1; 6.9 3.1; 5.8 2.7; 6.8 3.2; 6.7 3.3; 6.7 3.0; 6.3 2.5; 6.5 3.0; 6.2 3.4; 5.9 3.0]\nt = [0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2]\nt = Classification.CTOH(t)\nw = Classification.MS.fit(x, t, alpha = 0.1)\ncee = LossFunction.CEE(x, t, w, t_f = true)\nprintln(\"cee:\", cee)\n#52.20173706758349\n\n#CEE of OVR function\nw = Classification.OVR.fit(x, t; alpha = 0.1)\ncee = LossFunction.CEE(x, t, w, sigmoid_f = true)\nprintln(\"cee:\", cee)\n#28.702950289767305","category":"page"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"The arguments are as follows.","category":"page"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"LossFunction.CEE()","category":"page"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"- x : Matrix of dependent variables\r\n-t : Matrix of Objective variables\r\n- w : Model parameters obtained by fitting","category":"page"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"The following are keyword arguments. \t- meanf : Whether to divide the sum of the errors to get the average cross entropy error, true by default \t- sigmoidf : This argument must be true if you want the CEE of the OVR function. The default value is false \t- t_f : Specify the formula required by CEE. If true, it will be (Equation-1), if false, it will be (Equation-2). The default value is false","category":"page"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"(N is the number of elements, K is the number of dimensions of the input) (Equation-1) -frac1Nsum_n=0^N-1 t_nlogy_n+(1-t_n)log(1-y_n)","category":"page"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"(Equation-2) -frac1Nsum_n=0^N-1sum_k=0^K-1t_nklogy_nk","category":"page"},{"location":"Preprocessing/#Preprocessing","page":"Preprocessing","title":"Preprocessing","text":"","category":"section"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"All Preprocessing functions have the same arguments and specifications, so write only Standard Scaler.","category":"page"},{"location":"Preprocessing/#How-to-use-Standard-Scaler","page":"Preprocessing","title":"How to use Standard Scaler","text":"","category":"section"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Use the standard scaler (and other preprocessing functions) as follows.","category":"page"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"x = [1 2 3 4 5; 2 3 4 5 6]]\nsd = Preprocessing.SS.fit_transform(x, axis = 2)\nprintln(\"sd:\", sd)\n#sd:(Any[[-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518], [-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518]], Any[[3.0, 1.5811388300841898], [4.0, 1.5811388300841898]])\ninv = Preprocessing.SS.inverse_transform(sd[1], sd[2], axis = 2)\nprintln(\"inv:\", inv)\n#inv:Any[[1.0 2.0 3.0 4.0 5.0], [2.0 3.0 4.0 5.0 6.0]]\np = Preprocessing.SS.fit(x, axis = 2)\nprintln(\"fit:\", p)\n#fit:Any[[3.0, 1.5811388300841898], [4.0, 1.5811388300841898]]\np = Preprocessing.SS.transform(x, p, axis = 2)\nprintln(\"p:\", p)\n#p:Any[[-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518], [-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518]]","category":"page"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Using the fit function and then the transform function is completely equivalent to using the fittransform function. The fit function is a function that only calculates and returns the numerical value required for scaling, and the transform function scales using the numerical value obtained by fit. The fittransform function does all this at once. On the other hand, the inverse_transform function is a function that restores the scaled numerical value or the value predicted by the model to the original value.","category":"page"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Preprocessing.SS.fit_transform()","category":"page"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"- x : Matrix to scale\r\n- axis : Specify the axis to be scaled, axis = 1 if each feature is lined up in each column, and axis = 2 if each feature is lined up in each row. The default value is 1. This argument is a keyword argument","category":"page"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Preprocessing.SS.fit()","category":"page"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"- x : Matrix to scale\r\n- axis : Specify the axis to be scaled, axis = 1 if each feature is lined up in each column, and axis = 2 if each feature is lined up in each row. The default value is 1. This argument is a keyword argument","category":"page"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Preprocessing.SS.transform()","category":"page"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"- x : Matrix tp scale\r\n- p : Pass the number p obtained by the fit function and fit_transform function\r\n- axis :  Specify the axis to be scaled, axis = 1 if each feature is lined up in each column, and axis = 2 if each feature is lined up in each row. The default value is 1. This argument is a keyword argument","category":"page"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Preprocessing.SS.invers_transform()","category":"page"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"- x : Matrix to scale in reverse\r\n- p : Pass the number p obtained when scaling\r\n- axis : On the contrary, specify the axis to be scaled, axis = 1 if each feature is lined up in each column, and axis = 2 if each feature is lined up in each row. The default value is 1","category":"page"},{"location":"Preprocessing/#Other-functions","page":"Preprocessing","title":"Other functions","text":"","category":"section"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"MinMaxScaler can be used in Preprocessing.MM. MM scales based on the following formula: tildeboldsymbolx = fracboldsymbolx-min(boldsymbolx)max(boldsymbolx)-min(boldsymbolx)","category":"page"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"RobustScaler can be used with Preprocessing.RS. RS scales based on the following formula: tildeboldsymbolx = fracboldsymbolx-Q2Q3 - Q1","category":"page"},{"location":"Regression/#Regression","page":"Regression","title":"Regression","text":"","category":"section"},{"location":"Regression/#How-to-use-SGD","page":"Regression","title":"How to use SGD","text":"","category":"section"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Stochastic Gradient Descent(SGD) can be used as follows.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"using Horse\nx = [15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.1 26.95 5.68 21.76]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.71 173.29 159.31 171.52 138.96 165.87]\nw = Linear_Regression.SGD.fit(x, t)\nprintln(w) #[1.539947 136.176160] The first element is tilted and the second element is the intercept.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"The arguments are as follows.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Linear_Regression.SGD.fit()\nx : Dependent variable\nt : Objective variable","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"The following are keyword arguments.     - alpha : Learning rate. The default value is 0.001 \t- tau_max : Maximum number of iterations. The default variable is 100000 \t- eps : Absolute threshold of gradient to stop repeating. The default value is 0.1 The return value is an Array type.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Prediction using the built model can be done with the following function. However, x and w are not keyword arguments.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Linear_Regression.SGD.predict()","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"- x:Dependent variable\n- w:Array of obtained by fitting","category":"page"},{"location":"Regression/#How-to-use-multiple-regression","page":"Regression","title":"How to use multiple regression","text":"","category":"section"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Multiple regression can be used as follows.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"using Horse\nx =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\nw = Linear_Regression.MR.fit(x, t) #Here, because there is no inverse matrix, I get an error.\nprintln(w)","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"In multiple regression, if the inverse matrix cannot be generated in the program, the following error is returned.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Perhaps the matrix x you passed does not have an inverse matrix. In that case, use ridge regression.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"As shown in the error statement, regression can be performed by using ridge regression or lasso regression, but the eigenvalue is close to 0, which makes the model unstable. Therefore, it is one way to perform processing such as deleting a row of highly correlated data.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"The arguments are as follows. However, x and t are not keyword arguments.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Linear_Regression.MR.fit()","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"- x : Dependent variable\n- t : Objective variable","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"The return value is an Array type, the first element is the intercept of the model, and the subsequent elements are the coefficients of each dependent variable.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Prediction using the built model can be done with the following function. However, x and w are not keyword arguments.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Linear_Regression.MR.predict()","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"- x : Dependent variable\n- w : Array of w obtained by fitting","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"The return value is an Array type.","category":"page"},{"location":"Regression/#How-to-use-Ridge-Regression","page":"Regression","title":"How to use Ridge Regression","text":"","category":"section"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Ridge regression can be used as follows.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"using Horse\nx =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\nw = Linear_Regression.RR.fit(x, t)\n#[65.47458454192515; 0.1042034860770639; 1.5756199878279644] The first element represents the intercept and the second and subsequent elements represent the coefficients of the dependent variable.\nprintln(w)","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"The arguments are as follows.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Linear_Regression.RR.fit()","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"- x : Dependent variable\n- t : Objective variable","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"The following are keyword arguments. \t- alpha : learning rate. The deault value is 0.1 The return value is an Array type, the first element represents the intercept of the model, and the second and subsequent elements represent the coefficients of the dependent variable.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Prediction using the built model can be done with the following function. However, x and w are not keyword arguments.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Linear_Regression.RR.predict()","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"- x : Dependent variable\n- w : Array of obtained by fitting","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"The return value is an Array type.","category":"page"},{"location":"Regression/#How-to-use-Lasso-Regression","page":"Regression","title":"How to use Lasso Regression","text":"","category":"section"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Lasso regression can be used as follows.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"using Horse\nx =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\nw = Linear_Regression.LR.fit(x, t)\n# [144.61846209854855, 0.951158307335192, 0.0] The first element represents the intercept and the second and subsequent elements represent the coefficients of the dependent variable.\nprintln(w)","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"The arguments are as follows.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Linear_Regression.LR.fit()","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"- x : Dependent variable\n- t : Objective variable","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"The following are keyword arguments. \t- alpha : learning rate. The deault value is 0.1 \t- tol : The absolute value of the error to stop repeating. The deault value is 0.0001 \t- mi : Maximum number of iterations. The default value is 1000000 The return value is of type Array, the first element represents the intercept of the model, and the second and subsequent elements represent the coefficients of the dependent variable.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Prediction using the built model can be done with the following function. However, x and w are not keyword arguments.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Linear_Regression.LR.predict()","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"- x : Dependent variable\n- w : Array of abstained fitting","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"The return value is of type LinearAlgebra.Adjoint.","category":"page"},{"location":"Regression/#How-to-use-basis-function-model","page":"Regression","title":"How to use basis function model","text":"","category":"section"},{"location":"Regression/","page":"Regression","title":"Regression","text":"The basis function model can be used as follows.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"using LearningHorse\n x =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\n\n#Use Gaussian basis functions\nw, m, s = LinearRegression.BFM.fit(x[1, :], t, 4)\nprintln(\"w:\", w, \"m:\", m, \"s:\", s)\n#w:[-10.6733; 65.4475; -17.7623; 66.6557; 104.918]m:Any[5, 12.3333, 19.6667, 27.0]s:7.333333333333333\n\np = LinearRegression.BFM.predict(x[1, :], w, m, s)\nprintln(p)\n#[165.07, 168.55, 132.278, 162.546, 150.433, 144.04, 154.507, 164.011, 164.902, 165.417, 165.082, 167.712, 156.153, 169.583, 135.749, 167.366]\n\n#Use polynomial basis functions\nw, m, s = LinearRegression.BFM.fit(x[1, :], t, 4, alpha = 0.1, f = \"polynomial\")\nprintln(\"w, m, s(polynomial):\", w, m, s)\n#w, m, s(polynomial):[30.074721512969795; 17.649472753936088; -0.9665443355191238; 0.01733639801805678; 30.074721512971067]Any[5, 12.333333333333332, 19.666666666666664, 26.999999999999996]7.333333333333333\n\np = LinearRegression.BFM.predict(x[1, :], w, m, s, f = \"polynomial\")\nprintln(\"p(polynomial):\", p)\n#p(polynomial):[166.04905485192677, 165.72435351634755, 126.40024815990033, 163.70105505929791, 151.81467689141562, 144.29063632133014, 156.07721189214902, 165.05792844782184, 165.89978336872304, 165.6418324683704, 166.05887282906252, 165.26960817342376,157.70790163159944, 173.13926201582598, 132.39231072417851, 165.16779149696566]","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"The only functions are predict and fit, but you can use two models, the polynomial regression and the Gaussian basis function model, by specifying the keyword argument f. The arguments are as follows.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Function to learn.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"LinearRegression.BFM.fit()","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"- x : Dependent variable vector (matrix cannot be included) \n- t : Objective variable vector (matrix cannot be included)\n- M : Set the number of dimensions to fit, for example M = 2 for 2 dimensions.","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"The following are keyword arguments. \t- m : It can be specified for Gaussian basis functions. By default it uses the center vector of each Gaussian function \t- s : Like m, it can be specified for Gaussian functions. The default is to use a scalar that represents the spacing of each Gaussian function. \t- alpha : The coefficient applied to the regularization term that is added when calculating the design matrix. The default value is 0.0 \t- f : Specify the type of basis function, pass the string \"gauss\" for Gaussian functions, or \"polynomial\" for polynomial regression. Note that the default value is \"gauss\"","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Function to predict","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"LinearRegression.BFM.predict()","category":"page"},{"location":"Regression/","page":"Regression","title":"Regression","text":"- x : The vector of the dependent variable used for the prediction\n- w : Pass w of Array of the model obtained by fitting","category":"page"},{"location":"Classification/#Classification","page":"Classification","title":"Classification","text":"","category":"section"},{"location":"Classification/#How-to-use-Label-Encoder","page":"Classification","title":"How to use Label Encoder","text":"","category":"section"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Label Encoder can used as follows.","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"using LearningHorse\nlabel = [\"Apple\", \"Apple\", \"Pear\", \"Pear\", \"Lemon\", \"Apple\", \"Pear\", \"Lemon\"]\nl, d = Classification.LE(label)\nprintln(l)\n#[3.0 3.0 1.0 1.0 2.0 3.0 1.0 2.0]\nprintln(\"d:\", d)\n#d:Dict{Any, Any}(\"Pear\" => 1, \"Lemon\" => 2, \"Apple\" => 3)","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Since the label encoder is implemented using the Set type, the order of the elements is not guaranteed, so a Dict is returned indicating which class corresponds to which label. The arguments are as follows.","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Classification.LE()","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"- t : Array of labels to convert","category":"page"},{"location":"Classification/#Conversion-to-One-hot-format","page":"Classification","title":"Conversion to One-hot format","text":"","category":"section"},{"location":"Classification/","page":"Classification","title":"Classification","text":"The CTOH function that converts to One-hot format is used as follows. (However, this code is a continuation of the previous code.)","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"t = Classification.CTOH(l)\nprintln(\"t:\", t)\n#t:[0.0 0.0 1.0; 0.0 0.0 1.0; 1.0 0.0 0.0; 1.0 0.0 0.0; 0.0 1.0 0.0; 0.0 0.0 1.0; 1.0 0.0 0.0; 0.0 1.0 0.0]","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"This function can convert the class of the passed vector if it starts from 0, but if you pass a vector that represents the class with a negative number, an error will occur. The arguments are as follows.","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Classification.CTOH()","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"- t : A numerical vector representing the class to be converted.","category":"page"},{"location":"Classification/#Multi-class-classification-using-Multi-Class-Softmax","page":"Classification","title":"Multi-class classification using Multi Class Softmax","text":"","category":"section"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Classification using multi-class softmax using MS functions is performed as follows.","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"x = [5.1 3.5; 4.9 3.0; 4.7 3.2; 4.6 3.1; 5.0 3.6; 5.4 3.9; 4.6 3.4; 5.0 3.4; 4.4 2.9; 4.9 3.1; 5.4 3.7; 4.8 3.4; 4.8 3.0; 4.3 3.0; 5.8 4.0; 5.7 4.4; 5.4 3.9; 5.1 3.5; 5.7 3.8; 5.1 3.8; 5.4 3.4; 5.1 3.7; 4.6 3.6; 5.1 3.3; 4.8 3.4; 5.0 3.0; 5.0 3.4; 5.2 3.5; 5.2 3.4; 4.7 3.2; 4.8 3.1; 5.4 3.4; 5.2 4.1; 5.5 4.2; 4.9 3.1; 5.0 3.2; 5.5 3.5; 4.9 3.6; 4.4 3.0; 5.1 3.4; 5.0 3.5; 4.5 2.3; 4.4 3.2; 5.0 3.5; 5.1 3.8; 4.8 3.0; 5.1 3.8; 4.6 3.2; 5.3 3.7; 5.0 3.3; 7.0 3.2; 6.4 3.2; 6.9 3.1; 5.5 2.3; 6.5 2.8; 5.7 2.8; 6.3 3.3; 4.9 2.4; 6.6 2.9; 5.2 2.7; 5.0 2.0; 5.9 3.0; 6.0 2.2; 6.1 2.9; 5.6 2.9; 6.7 3.1; 5.6 3.0; 5.8 2.7; 6.2 2.2; 5.6 2.5; 5.9 3.2; 6.1 2.8; 6.3 2.5; 6.1 2.8; 6.4 2.9; 6.6 3.0; 6.8 2.8; 6.7 3.0; 6.0 2.9; 5.7 2.6; 5.5 2.4; 5.5 2.4; 5.8 2.7; 6.0 2.7; 5.4 3.0; 6.0 3.4; 6.7 3.1; 6.3 2.3; 5.6 3.0; 5.5 2.5; 5.5 2.6; 6.1 3.0; 5.8 2.6; 5.0 2.3; 5.6 2.7; 5.7 3.0; 5.7 2.9; 6.2 2.9; 5.1 2.5; 5.7 2.8; 6.3 3.3; 5.8 2.7; 7.1 3.0; 6.3 2.9; 6.5 3.0; 7.6 3.0; 4.9 2.5; 7.3 2.9; 6.7 2.5; 7.2 3.6; 6.5 3.2; 6.4 2.7; 6.8 3.0; 5.7 2.5; 5.8 2.8; 6.4 3.2; 6.5 3.0; 7.7 3.8; 7.7 2.6; 6.0 2.2; 6.9 3.2; 5.6 2.8; 7.7 2.8; 6.3 2.7; 6.7 3.3; 7.2 3.2; 6.2 2.8; 6.1 3.0; 6.4 2.8; 7.2 3.0; 7.4 2.8; 7.9 3.8; 6.4 2.8; 6.3 2.8; 6.1 2.6; 7.7 3.0; 6.3 3.4; 6.4 3.1; 6.0 3.0; 6.9 3.1; 6.7 3.1; 6.9 3.1; 5.8 2.7; 6.8 3.2; 6.7 3.3; 6.7 3.0; 6.3 2.5; 6.5 3.0; 6.2 3.4; 5.9 3.0]\nt = [0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2]\nt = Classification.CTOH(t)\nw = Classification.MS.fit(x, t, alpha = 0.1)\nprintln(w)\n#[1.7209083498295872 1.8953943405297293 -0.6163026903593205; -1.256172465412238 1.8287641755675748 2.4274082898446574; 4.812985176776389 -0.7076002068908029 -1.1053849698855869]\np = Classification.MS.predict(x, w)\nprintln(p)\n#[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 1, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2]","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"The arguments are as follows.","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Functions that perform learning.","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Classification.MS.fit()","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"- x : Matrix of dependent variables\r\n- t : Matrix of objective variables of a class formatted using LE or CTOH (Array)","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"The following are keyword arguments. \t- alpha : Learning rate when searching for the optimum parameter by the steepest gradient method. The default value is 0.01 \t- tau_max : Maximum number of iterations when optimizing parameters. The default value is 1000","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Function to make a prediction.","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Classification.MS.predict()","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"- x : Matrix of dependent variables used for prediction\r\n- w : Array of parameters w obtained by fitting","category":"page"},{"location":"Classification/#Multiclass-classification-using-One-vs-Rest","page":"Classification","title":"Multiclass classification using One-vs-Rest","text":"","category":"section"},{"location":"Classification/","page":"Classification","title":"Classification","text":"One-vs-Rest can be used as follows. (This code is a continuation of the previous Multiclass Softmax.)","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"w = Classification.OVR.fit(x, t; alpha = 0.1)\nprintln(\"w:\", w)\n#w:Any[[1.4720211645121106 -3.049021804351106 4.768579857703322], [1.496138559080458 0.5690372319250491 -1.8729783593510938], [-1.2013066350855492 1.258936673691226 -2.3333697507254283]]\np = Classification.OVR.predict(x, w)\nprintln(\"predict:\", p)\n#predict:Any[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 1, 1, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3,3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2]","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"The arguments are as follows.","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Function that preform learning.","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Classification.OVR.fit()","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"- x : Dependent variable matrix (Array)\r\n- t : Matrix of objective variables of a class formatted using LE or CTOH (Array)","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"The following are keyword arguments. \t- alpha : Learning rate when searching for the optimum parameter by the steepest gradient method. The default value is 0.01 \t- tau_max : Maximum number of iterations when optimizing parameters. The default value is 1000","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Function to make a prediction.","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Classification.OVR.predict()","category":"page"},{"location":"Classification/","page":"Classification","title":"Classification","text":"- x : Matrix of dependent variables used for prediction\r\n- w : Array of parameters w obtained by fitting","category":"page"},{"location":"#LearningHorse.jl","page":"Home","title":"LearningHorse.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LearningHorse provides an easy-to-use machine learning library","category":"page"},{"location":"#Resources","page":"Home","title":"Resources","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation:https://ik1-414-39231.vs.sakura.ne.jp/learninghorse/\nSouce code:https://github.com/QGMW22/Learninghorse.jl","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LearningHorse can be installed using the Julia package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and run.","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg > add LearningHorse","category":"page"},{"location":"#Improvement","page":"Home","title":"Improvement","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Next, we plan to implement Ensemble method, etc.","category":"page"},{"location":"Japanese/#LearningHorse.jl-日本語版マニュアル","page":"日本語版マニュアル","title":"LearningHorse.jl 日本語版マニュアル","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Horseは使いやすく、高速な機械学習ライブラリを提供します。現時点では、回帰による学習に限られていますが、これから徐々に拡張していきます。","category":"page"},{"location":"Japanese/#インストール","page":"日本語版マニュアル","title":"インストール","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Horseは今のところ名前でインストールする事はできません、次のようにする必要があります。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"pkg > add https://github.com/QGMW22/Horse/Horse.jl","category":"page"},{"location":"Japanese/#使い方","page":"日本語版マニュアル","title":"使い方","text":"","category":"section"},{"location":"Japanese/#回帰の関数","page":"日本語版マニュアル","title":"回帰の関数","text":"","category":"section"},{"location":"Japanese/#SGDの使い方","page":"日本語版マニュアル","title":"SGDの使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"勾配最急下法（SGD）は以下のようにして使うことができます","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\nx = [15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.1 26.95 5.68 21.76]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.71 173.29 159.31 171.52 138.96 165.87]\nw = Linear_Regression.SGD.fit(x, t)\nprintln(w) #[1.539947 136.176160] 一つ目の要素が傾き、二つ目の要素が切片を表す。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数は次の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.SGD.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列（ただし一次元）\n- t：目的変数の行列","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"以下キーワード引数 \t- alpha：学習率、デフォルト値は0.001 \t- tau_max：繰り返しの最大回数、デフォルト値は100000 \t- eps：繰り返しをやめる勾配の絶対値の閾値、デフォルト値は0.1","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"返り値は、Array型で返されます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"構築したモデルを使った予測は以下の関数で行えます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.SGD.predict()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数\n- w：フィットして得られたwのArray","category":"page"},{"location":"Japanese/#重回帰の使い方","page":"日本語版マニュアル","title":"重回帰の使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"重回帰は以下のようにして使うことができます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\nx =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\nw = Linear_Regression.MR.fit(x, t) #ここでは、逆行列が存在しないため、Errorが出る\nprintln(w)","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"重回帰では、プログラム内で逆行列を生成できなかった場合、以下のようなErrorを返します。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Perhaps the matrix x you passed does not have an inverse matrix. In that case, use ridge regression.","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"エラー文の通りリッジ回帰を使うのも一つの手段ではありますが、いずれにしても固有値が0に近いことは事実であるが故に、不安定なモデルになってしまうため、相関の高いデータを一列残して削除することで解決する等の手段も考ると良いです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.MR.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列\n- t：目的変数の行列","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"返り値はArray型で返され、一つ目の要素がモデルの切片、それ以降はそれぞれの従属変数の係数となっています。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"構築したモデルを使った予測は以下の関数で行えます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.MR.predict()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列\n- w：フィットして得られたwのArray","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"返り値はArray型で返されます。","category":"page"},{"location":"Japanese/#リッジ回帰の使い方","page":"日本語版マニュアル","title":"リッジ回帰の使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"リッジ回帰は以下のようにして使うことができます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\nx =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\nw = Linear_Regression.RR.fit(x, t)\nprintln(w) #[65.47458454192515; 0.1042034860770639; 1.5756199878279644] 一つ目の要素が切片、二つ以降の要素は従属変数xの係数を表す","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.RR.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列\n- t：目的変数の行列","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"キーワード引数 \t- alpha：学習率、デフォルト値は0.1 戻り値はArray型で返され、一つ目の要素がモデルの切片、二つ目以降の要素は従属変数xの係数を表します。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"構築したモデルを使った予測は以下の関数で行えます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.RR.predict()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列\n- w：フィットして得たwのArray","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"返り値はArray型で返されます。","category":"page"},{"location":"Japanese/#ラッソ回帰の使い方","page":"日本語版マニュアル","title":"ラッソ回帰の使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"ラッソ回帰は以下のようにして使うことができます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\nx =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\nw = Linear_Regression.LR.fit(x, t)\nprintln(w) # [144.61846209854855, 0.951158307335192, 0.0] 一つ目の要素が切片、二つ目以降が従属変数xの係数を表す","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.LR.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列\n- t：目的変数の行列","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"以下キーワード引数 \t- alpha：学習率、デフォルト値は0.1 \t- tol：繰り返しをやめる誤差の絶対値の闘値、デフォルト値は0.0001 \t- mi：繰り返しの最大回数、デフォルト値は1000000 戻り値はArray型で返され、一つ目の要素はモデルの切片、二つ目以降が従属変数xの係数を表します。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"構築したモデルを使った予測は以下の関数で行えます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.LR.predict()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数\n- w：フィットして得られたwのArray","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"返り値はLinearAlgebra.Adjoint型で返されます。","category":"page"},{"location":"Japanese/#基底関数モデルの使い方","page":"日本語版マニュアル","title":"基底関数モデルの使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"基底関数モデルは以下のようにして使うことができます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\n x =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\n\n#ガウス基底関数を使用\nw, m, s = LinearRegression.BFM.fit(x[1, :], t, 4)\nprintln(\"w:\", w, \"m:\", m, \"s:\", s)\n#w:[-10.6733; 65.4475; -17.7623; 66.6557; 104.918]m:Any[5, 12.3333, 19.6667, 27.0]s:7.333333333333333\n\np = LinearRegression.BFM.predict(x[1, :], w, m, s)\nprintln(p)\n#[165.07, 168.55, 132.278, 162.546, 150.433, 144.04, 154.507, 164.011, 164.902, 165.417, 165.082, 167.712, 156.153, 169.583, 135.749, 167.366]\n\n#多項式基底関数を使用\nw, m, s = LinearRegression.BFM.fit(x[1, :], t, 4, alpha = 0.1, f = \"polynomial\")\nprintln(\"w, m, s(polynomial):\", w, m, s)\n#w, m, s(polynomial):[30.074721512969795; 17.649472753936088; -0.9665443355191238; 0.01733639801805678; 30.074721512971067]Any[5, 12.333333333333332, 19.666666666666664, 26.999999999999996]7.333333333333333\n\np = LinearRegression.BFM.predict(x[1, :], w, m, s, f = \"polynomial\")\nprintln(\"p(polynomial):\", p)\n#p(polynomial):[166.04905485192677, 165.72435351634755, 126.40024815990033, 163.70105505929791, 151.81467689141562, 144.29063632133014, 156.07721189214902, 165.05792844782184, 165.89978336872304, 165.6418324683704, 166.05887282906252, 165.26960817342376,157.70790163159944, 173.13926201582598, 132.39231072417851, 165.16779149696566]","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"関数は、predictとfitのみですが、キーワード引数 f を指定することで、多項式回帰とガウス基底関数モデルの二つのモデルを使うことができます。 引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"学習する関数","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"LinearRegression.BFM.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数のベクトル（行列を入れることはできません）\n- t：目的変数のベクトル（行列を入れることはできません）\n- M：フィットする次元数、2次元ならM = 2とします。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"以下、キーワード引数 \t- m：ガウス基底関数の場合に指定することができます。デフォルトでは各ガウス関数の中心のベクトルを使用します \t- s：mと同様、ガウス関数の場合に指定することができます。デフォルトでは各ガウス関数の間隔を表すスカラーを使用します \t- alpha：計画行列の計算を行う際に加算される正則化項にかけられる係数です。デフォルト値は0.0 \t- f：基底関数の種類の指定、ガウス関数は文字列\"gauss\"、多項式回帰の場合は\"polynomial\"を渡します。デフォルト値は\"gauss\"であることに注意してください","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"予測する関数","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"LinearRegression.BFM.predict()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：予測に使用する従属変数のベクトル\n- w：フィットして得られたモデルのArrayのwを渡す","category":"page"},{"location":"Japanese/#分類の関数","page":"日本語版マニュアル","title":"分類の関数","text":"","category":"section"},{"location":"Japanese/#ラベルエンコーダーの使い方","page":"日本語版マニュアル","title":"ラベルエンコーダーの使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"ラベルエンコーダーは以下のように使います。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\nlabel = [\"Apple\", \"Apple\", \"Pear\", \"Pear\", \"Lemon\", \"Apple\", \"Pear\", \"Lemon\"]\nl, d = Classification.LE(label)\nprintln(l)\n#[3.0 3.0 1.0 1.0 2.0 3.0 1.0 2.0]\nprintln(\"d:\", d)\n#d:Dict{Any, Any}(\"Pear\" => 1, \"Lemon\" => 2, \"Apple\" => 3)","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"ラベルエンコーダーはSet型を使用して実装されているため要素の順番は保証されません、そのため、どのクラスがどのラベルに対応するのかを示すDictが返されます。引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Classification.LE()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- t：変換するラベルのArray","category":"page"},{"location":"Japanese/#One-Hot形式への変換","page":"日本語版マニュアル","title":"One Hot形式への変換","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"One Hot形式への変換を行うCTOH関数は以下のようにして使います。（ただしこのコードは先ほどのコードの続きとします。）","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"t = Classification.CTOH(l)\nprintln(\"t:\", t)\n#t:[0.0 0.0 1.0; 0.0 0.0 1.0; 1.0 0.0 0.0; 1.0 0.0 0.0; 0.0 1.0 0.0; 0.0 0.0 1.0; 1.0 0.0 0.0; 0.0 1.0 0.0]","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"この関数は渡されるベクトルのクラスの表し方が、0から始まるものは変換可能ですが、クラスを負の数で表したベクトルを渡すとエラーとなリます。 引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Classification.CTOH()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- t：変換するクラスを数値で表したベクトル。","category":"page"},{"location":"Japanese/#マルチクラスソフトマックスを使用したマルチクラス分類","page":"日本語版マニュアル","title":"マルチクラスソフトマックスを使用したマルチクラス分類","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"MS関数を使用したマルチクラスソフトマックスを使用した分類は以下のように行います。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"x = [5.1 3.5; 4.9 3.0; 4.7 3.2; 4.6 3.1; 5.0 3.6; 5.4 3.9; 4.6 3.4; 5.0 3.4; 4.4 2.9; 4.9 3.1; 5.4 3.7; 4.8 3.4; 4.8 3.0; 4.3 3.0; 5.8 4.0; 5.7 4.4; 5.4 3.9; 5.1 3.5; 5.7 3.8; 5.1 3.8; 5.4 3.4; 5.1 3.7; 4.6 3.6; 5.1 3.3; 4.8 3.4; 5.0 3.0; 5.0 3.4; 5.2 3.5; 5.2 3.4; 4.7 3.2; 4.8 3.1; 5.4 3.4; 5.2 4.1; 5.5 4.2; 4.9 3.1; 5.0 3.2; 5.5 3.5; 4.9 3.6; 4.4 3.0; 5.1 3.4; 5.0 3.5; 4.5 2.3; 4.4 3.2; 5.0 3.5; 5.1 3.8; 4.8 3.0; 5.1 3.8; 4.6 3.2; 5.3 3.7; 5.0 3.3; 7.0 3.2; 6.4 3.2; 6.9 3.1; 5.5 2.3; 6.5 2.8; 5.7 2.8; 6.3 3.3; 4.9 2.4; 6.6 2.9; 5.2 2.7; 5.0 2.0; 5.9 3.0; 6.0 2.2; 6.1 2.9; 5.6 2.9; 6.7 3.1; 5.6 3.0; 5.8 2.7; 6.2 2.2; 5.6 2.5; 5.9 3.2; 6.1 2.8; 6.3 2.5; 6.1 2.8; 6.4 2.9; 6.6 3.0; 6.8 2.8; 6.7 3.0; 6.0 2.9; 5.7 2.6; 5.5 2.4; 5.5 2.4; 5.8 2.7; 6.0 2.7; 5.4 3.0; 6.0 3.4; 6.7 3.1; 6.3 2.3; 5.6 3.0; 5.5 2.5; 5.5 2.6; 6.1 3.0; 5.8 2.6; 5.0 2.3; 5.6 2.7; 5.7 3.0; 5.7 2.9; 6.2 2.9; 5.1 2.5; 5.7 2.8; 6.3 3.3; 5.8 2.7; 7.1 3.0; 6.3 2.9; 6.5 3.0; 7.6 3.0; 4.9 2.5; 7.3 2.9; 6.7 2.5; 7.2 3.6; 6.5 3.2; 6.4 2.7; 6.8 3.0; 5.7 2.5; 5.8 2.8; 6.4 3.2; 6.5 3.0; 7.7 3.8; 7.7 2.6; 6.0 2.2; 6.9 3.2; 5.6 2.8; 7.7 2.8; 6.3 2.7; 6.7 3.3; 7.2 3.2; 6.2 2.8; 6.1 3.0; 6.4 2.8; 7.2 3.0; 7.4 2.8; 7.9 3.8; 6.4 2.8; 6.3 2.8; 6.1 2.6; 7.7 3.0; 6.3 3.4; 6.4 3.1; 6.0 3.0; 6.9 3.1; 6.7 3.1; 6.9 3.1; 5.8 2.7; 6.8 3.2; 6.7 3.3; 6.7 3.0; 6.3 2.5; 6.5 3.0; 6.2 3.4; 5.9 3.0]\nt = [0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2]\nt = Classification.CTOH(t)\nw = Classification.MS.fit(x, t, alpha = 0.1)\nprintln(w)\n#[1.7209083498295872 1.8953943405297293 -0.6163026903593205; -1.256172465412238 1.8287641755675748 2.4274082898446574; 4.812985176776389 -0.7076002068908029 -1.1053849698855869]\np = Classification.MS.predict(x, w)\nprintln(p)\n#[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 1, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2]","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"学習を行う関数","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Classification.MS.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列（Array）\n- t：LEやCTOHを使用して整形したクラスの目的変数の行列（Array）","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"以下はキーワード引数 \t- alpha：最適なパラメータを勾配最急下法で探索する際の学習率、デフォルト値は0.01 \t- tau_max：alphaと同じくパラメータを最適化する際の最大繰り返し回数、デフォルト値は1000","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"予測を行う関数","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Classification.MS.predict()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：予測に使用する従属変数の行列\n- w：フィットして得られたパラメータwのArray","category":"page"},{"location":"Japanese/#One-vs-Restを使用したマルチクラス分類","page":"日本語版マニュアル","title":"One-vs-Restを使用したマルチクラス分類","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"One-vs-Restは以下のようにして使用することができます。（このコードは先ほどのマルチクラスソフトマックスの続きとします。）","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"w = Classification.OVR.fit(x, t; alpha = 0.1)\nprintln(\"w:\", w)\n#w:Any[[1.4720211645121106 -3.049021804351106 4.768579857703322], [1.496138559080458 0.5690372319250491 -1.8729783593510938], [-1.2013066350855492 1.258936673691226 -2.3333697507254283]]\np = Classification.OVR.predict(x, w)\nprintln(\"predict:\", p)\n#predict:Any[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 1, 1, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3,3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2]","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"学習を行う関数","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Classification.OVR.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列（Array）\n- t：LEやCTOHを使用して整形したクラスの目的変数の行列（Array）","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"以下はキーワード引数 \t- alpha：最適なパラメータを勾配最急下法で探索する際の学習率、デフォルト値は0.01 \t- tau_max：alphaと同じくパラメータを最適化する際の最大繰り返し回数、デフォルト値は1000","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"予測を行う関数","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Classification.OVR.predict()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：予測に使用する従属変数の行列\n- w：fitで得られたパラメータwのArray","category":"page"},{"location":"Japanese/#損失関数","page":"日本語版マニュアル","title":"損失関数","text":"","category":"section"},{"location":"Japanese/#MSE（平均二乗誤差）の使い方","page":"日本語版マニュアル","title":"MSE（平均二乗誤差）の使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"平均二乗誤差は以下のようにして使うことができます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\nmean = Loss_Function.MSE([1, 2, 3, 4], [1, 2, 3, 4], [1, 2])\nprintln(mean) #4\n\nx = [15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\n\n#ガウス基底関数の平均二乗誤差\nw, m, s = LinearRegression.BFM.fit(x[1, :], t, 4)\nmse = LossFunction.MSE(x[1, :], t, w, b = \"gauss\", m = m, s = s)\nprintln(\"mse(gauss):\", mse)\n#mse(gauss):16.777305554499097\n\n#多項式回帰の平均二乗誤差\nw, m, s = LinearRegression.BFM.fit(x[1, :], t, 4, alpha = 0.1, f = \"polynomial\")\nmse = LossFunction.MSE(x[1, :], t, w, b = \"polynomial\", m = m, s = s)\nprintln(\"mse(polynomial):\", mse)\n#mse(polynomial):18.491924125591847","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数bを指定することによって、基底関数モデルのMSEの算出が可能です。 引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"LossFunction.MSE()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列\n- t：目的変数の行列\n- w：フィットして得られたモデルのパラメータ","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"以下キーワード引数 \t- b：基底関数のMSEを算出する際に指定する。ガウス基底関数なら\"gauss\"、多項式回帰なら\"polynomial\" \t- m：ガウス基底関数のMSEを算出する際に指定できる。デフォルトでは各ガウス関数の中心のベクトルを使用して計算 \t- s：ガウス関数のMSEを算出する際に指定できる。デフォルトでは各ガウス関数の間隔を使用して計算 \t- mean_f：二乗して足し合わせた誤差の総和を割って、平均二乗誤差にするかどうか、デフォルト値はtrue","category":"page"},{"location":"Japanese/#CEE（交差エントロピー誤差）の使い方","page":"日本語版マニュアル","title":"CEE（交差エントロピー誤差）の使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"交差エントロピー誤差は以下のようにして使います。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\n#MSのCEE\nx = [5.1 3.5; 4.9 3.0; 4.7 3.2; 4.6 3.1; 5.0 3.6; 5.4 3.9; 4.6 3.4; 5.0 3.4; 4.4 2.9; 4.9 3.1; 5.4 3.7; 4.8 3.4; 4.8 3.0; 4.3 3.0; 5.8 4.0; 5.7 4.4; 5.4 3.9; 5.1 3.5; 5.7 3.8; 5.1 3.8; 5.4 3.4; 5.1 3.7; 4.6 3.6; 5.1 3.3; 4.8 3.4; 5.0 3.0; 5.0 3.4; 5.2 3.5; 5.2 3.4; 4.7 3.2; 4.8 3.1; 5.4 3.4; 5.2 4.1; 5.5 4.2; 4.9 3.1; 5.0 3.2; 5.5 3.5; 4.9 3.6; 4.4 3.0; 5.1 3.4; 5.0 3.5; 4.5 2.3; 4.4 3.2; 5.0 3.5; 5.1 3.8; 4.8 3.0; 5.1 3.8; 4.6 3.2; 5.3 3.7; 5.0 3.3; 7.0 3.2; 6.4 3.2; 6.9 3.1; 5.5 2.3; 6.5 2.8; 5.7 2.8; 6.3 3.3; 4.9 2.4; 6.6 2.9; 5.2 2.7; 5.0 2.0; 5.9 3.0; 6.0 2.2; 6.1 2.9; 5.6 2.9; 6.7 3.1; 5.6 3.0; 5.8 2.7; 6.2 2.2; 5.6 2.5; 5.9 3.2; 6.1 2.8; 6.3 2.5; 6.1 2.8; 6.4 2.9; 6.6 3.0; 6.8 2.8; 6.7 3.0; 6.0 2.9; 5.7 2.6; 5.5 2.4; 5.5 2.4; 5.8 2.7; 6.0 2.7; 5.4 3.0; 6.0 3.4; 6.7 3.1; 6.3 2.3; 5.6 3.0; 5.5 2.5; 5.5 2.6; 6.1 3.0; 5.8 2.6; 5.0 2.3; 5.6 2.7; 5.7 3.0; 5.7 2.9; 6.2 2.9; 5.1 2.5; 5.7 2.8; 6.3 3.3; 5.8 2.7; 7.1 3.0; 6.3 2.9; 6.5 3.0; 7.6 3.0; 4.9 2.5; 7.3 2.9; 6.7 2.5; 7.2 3.6; 6.5 3.2; 6.4 2.7; 6.8 3.0; 5.7 2.5; 5.8 2.8; 6.4 3.2; 6.5 3.0; 7.7 3.8; 7.7 2.6; 6.0 2.2; 6.9 3.2; 5.6 2.8; 7.7 2.8; 6.3 2.7; 6.7 3.3; 7.2 3.2; 6.2 2.8; 6.1 3.0; 6.4 2.8; 7.2 3.0; 7.4 2.8; 7.9 3.8; 6.4 2.8; 6.3 2.8; 6.1 2.6; 7.7 3.0; 6.3 3.4; 6.4 3.1; 6.0 3.0; 6.9 3.1; 6.7 3.1; 6.9 3.1; 5.8 2.7; 6.8 3.2; 6.7 3.3; 6.7 3.0; 6.3 2.5; 6.5 3.0; 6.2 3.4; 5.9 3.0]\nt = [0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2]\nt = Classification.CTOH(t)\nw = Classification.MS.fit(x, t, alpha = 0.1)\ncee = LossFunction.CEE(x, t, w, t_f = true)\nprintln(\"cee:\", cee)\n#52.20173706758349\n\n#OVRのCEE\nw = Classification.OVR.fit(x, t; alpha = 0.1)\ncee = LossFunction.CEE(x, t, w, sigmoid_f = true)\nprintln(\"cee:\", cee)\n#28.702950289767305","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"LossFunction.CEE()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列\n- t：目的変数の行列\n- w：フィットして得られたモデルのパラメータ","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"以下キーワード引数 \t- meanf：誤差の和を割って平均交差エントロピー誤差にするかどうか、デフォルトではtrue \t- sigmoidf：OVRでCEEを求める場合は、この引数をtrueにする必要があります。デフォルト値はfalse \t- t_f：CEEの求める式を指定します。trueなら（式−1）に、falseなら（式-2）になります。デフォルト値はfalse","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"（Nは要素の数、Kは入力の次元数） （式-1）-frac1Nsum_n=0^N-1 t_nlogy_n+(1-t_n)log(1-y_n)","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"（式-2）-frac1Nsum_n=0^N-1sum_k=0^K-1t_nklogy_nk","category":"page"},{"location":"Japanese/#Preprocessing","page":"日本語版マニュアル","title":"Preprocessing","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Preprocessingの関数は、全て引数や仕様が同じなので、Standard Scalerのみ書きます。","category":"page"},{"location":"Japanese/#Standard-Scalerの使い方","page":"日本語版マニュアル","title":"Standard Scalerの使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"StandardScaler（及びその他のPreprocessingの関数）は以下のように使います。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"x = [1 2 3 4 5; 2 3 4 5 6]]\nsd = Preprocessing.SS.fit_transform(x, axis = 2)\nprintln(\"sd:\", sd)\n#sd:(Any[[-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518], [-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518]], Any[[3.0, 1.5811388300841898], [4.0, 1.5811388300841898]])\ninv = Preprocessing.SS.inverse_transform(sd[1], sd[2], axis = 2)\nprintln(\"inv:\", inv)\n#inv:Any[[1.0 2.0 3.0 4.0 5.0], [2.0 3.0 4.0 5.0 6.0]]\np = Preprocessing.SS.fit(x, axis = 2)\nprintln(\"fit:\", p)\n#fit:Any[[3.0, 1.5811388300841898], [4.0, 1.5811388300841898]]\np = Preprocessing.SS.transform(x, p, axis = 2)\nprintln(\"p:\", p)\n#p:Any[[-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518], [-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518]]","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"fit関数を使ったのち、transform関数を使うのと、fittransform関数を使うのは完全に等価です。 fit関数は、スケーリングに必要な数値を算出し返すだけの関数で、fitで得られた数値を使ってスケーリングするのがtransform関数です。fittransform関数はこれらの処理を一度に行います。 一方、invers_transform関数は、スケーリングした数値やモデルで予測した値を元の値に戻す関数です。 引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Preprocessing.SS.fit_transform()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：スケーリングする行列\n- axis：スケーリングする軸、各特徴量が各列に並んでいればaxis = 1、各行に並んでいればaxis = 2を指定する。デフォルト値は1","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Preprocessing.SS.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：スケーリングする行列\n-axis：スケーリングする軸、各特徴量が各列に並んでいればaxis = 1、各行に並んでいればaxis = 2を指定する。デフォルト値は1","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Preprocessing.SS.transform()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：スケーリングする行列\n- p：fit関数やfit_transform関数で得られた数値pを渡す\n- axis：スケーリングする軸、各特徴量が各列に並んでいればaxis = 1、各行に並んでいればaxis = 2を指定する。デフォルト値は1","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Preprocessing.SS.invers_transform()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：逆にスケーリングする行列\n- p：スケーリングする際に求められた数値pを渡す\n- axis：逆にスケーリングする軸、各特徴量が各列に並んでいればaxis = 1、各行に並んでいればaxis = 2を指定する。デフォルト値は1","category":"page"},{"location":"Japanese/#その他の関数","page":"日本語版マニュアル","title":"その他の関数","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"MinMaxScalerはPreprocessing.MMで使うことができます。 MMは以下のような式に基づいてスケーリングします。 tildeboldsymbolx = fracboldsymbolx-min(boldsymbolx)max(boldsymbolx)-min(boldsymbolx)","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"RobustScalerはPreprocessing.RSで使うことができます。 RSは以下のような式に基づいてスケーリングします。 tildeboldsymbolx = fracboldsymbolx-Q2Q3 - Q1","category":"page"}]
}
