var documenterSearchIndex = {"docs":
[{"location":"LossFunction/#LossFunction","page":"LossFunction","title":"LossFunction","text":"","category":"section"},{"location":"LossFunction/","page":"LossFunction","title":"LossFunction","text":"LossFunction.mse\nLossFunction.cee","category":"page"},{"location":"LossFunction/#LearningHorse.LossFunction.mse","page":"LossFunction","title":"LearningHorse.LossFunction.mse","text":"mse(y, t)\n\nReturn the mean square error:mean((y .- t) .^ 2)\n\nExample\n\njulia> loss = mse\nmse (generic function with 1 method)\n\njulia> y, t = [1, 3, 2, 5], [1, 2, 3, 4]\n([1, 3, 2, 5], [1, 2, 3, 4])\n\njulia> loss(y, t)\n0.75\n\n\n\n\n\n","category":"function"},{"location":"LossFunction/#LearningHorse.LossFunction.cee","page":"LossFunction","title":"LearningHorse.LossFunction.cee","text":"cee(y, t, dims = 1)\n\nReturn the cross entropy error:mean(-sum(t.*log(y)))\n\nExample\n\njulia> loss = cee\ncee (generic function with 1 method)\n\njulia> y, t = [1, 3, 2, 5], [1, 2, 3, 4]\n([1, 3, 2, 5], [1, 2, 3, 4])\n\njulia> loss(y, t)\n-10.714417768752456\n\n\n\n\n\n","category":"function"},{"location":"Preprocessing/#Preprocessing","page":"Preprocessing","title":"Preprocessing","text":"","category":"section"},{"location":"Preprocessing/#Data-Preprocessing","page":"Preprocessing","title":"Data Preprocessing","text":"","category":"section"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Preprocessing.dataloader\nPreprocessing.DataSplitter","category":"page"},{"location":"Preprocessing/#LearningHorse.Preprocessing.dataloader","page":"Preprocessing","title":"LearningHorse.Preprocessing.dataloader","text":"dataloader(name; header=true, dir=learninghorsedatasets)\n\nLoad a data for Machine Learning. name is either the name of the datasets or the full path of the data file to be loaded. The following three can be specified as the name of the dataset in name.\n\nMNIST : The MNIST Datasets\niris : The iris Datasets\nBostonHousing : Boston Housing DataSets\n\nAnd these datasets are downloaded and saved by creating a dir folder under the home directly(i.e. it is saved in the {homedirectly}/learninghorsedatasets by default). When importing a data file, you can specify whether to read the header with header.\n\nExample\n\njulia> dataloader(\"MNIST\");\n\njulia> dataloader(\"/home/ubuntu/data/data.csv\", header = false)\n\n\n\n\n\n","category":"function"},{"location":"Preprocessing/#LearningHorse.Preprocessing.DataSplitter","page":"Preprocessing","title":"LearningHorse.Preprocessing.DataSplitter","text":"DataSplitter(ndata; test_size=nothing, train_size=nothing)\n\nSplit the data into test data and training data. ndata is the number of the data, and you must specify either test_size or train_size. thease parameter can be proprtional or number of data.\n\nnote: Note\nIf both test_size and train_size are specified, test_size takes precedence.\n\n#Example\n\njulia> x = rand(20, 2);\n\njulia> DS = DataSplitter(50, train_size = 0.3);\n\njulia> DS(x, dims = 2) |> size\n(14, 2)\n\n\n\n\n\n","category":"type"},{"location":"Preprocessing/#Scaler","page":"Preprocessing","title":"Scaler","text":"","category":"section"},{"location":"Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Preprocessing.Standard\nPreprocessing.MinMax\nPreprocessing.Robust\nPreprocessing.fit_transform!\nPreprocessing.inv_transform!","category":"page"},{"location":"Preprocessing/#LearningHorse.Preprocessing.Standard","page":"Preprocessing","title":"LearningHorse.Preprocessing.Standard","text":"Standard()\n\nStandard Scaler. This scaler scale data as: z_i = fracx_i-musigma\n\nExample\n\njulia> x = [\n    16.862463771320925 68.10823385851712\n    15.382965696961577 65.4313485700859\n    8.916228406218375 53.92034559524475\n    10.560285659132695 59.17305391117168\n    12.142253214135884 62.28708207525656\n    5.362107221163482 43.604947901567414\n    13.893239446341777 62.44348617377496\n    11.871357065173395 60.28433066289655\n    29.83792267802442 69.22281924803998\n    21.327107214235483 70.15810991597944\n    23.852372696012498 69.81780163668844\n    26.269031430914108 67.61037566099782\n    22.78907104644012 67.78105545358633\n    26.73342178134947 68.59263965946904\n    9.107259141706415 56.565383817343495\n    29.38551885863976 68.1005579469209\n    7.935966787763017 53.76264777936664\n    29.01677894379809 68.69484161138638\n    6.839609488194577 49.69794758177567\n    13.95215840314148 62.058116579899085]; #These data are also used to explanations of other functions.\n\njulia> t = [169.80980778351542, 167.9081124078835, 152.30845618985222, 160.3110300206261, 161.96826472170756, 136.02842285615077, 163.98131131382686, 160.117817321485, 172.22758529098235, 172.21342437006865, 171.8939175591617, 169.83018083884602, 171.3878062674257, 170.52487535026015, 156.40282783981309, 170.6488327896672, 151.69267899906185, 172.32478221316322, 145.14365314788827, 163.79383292080666];\n\njulia> scaler = Standard()\nStandard(Float64[])\n\njulia> fit!(scaler, x)\n2×2 Matrix{Float64}:\n 19.0591   64.467\n  6.95818   6.68467\n\njulia> transform!(scaler, x)\n20×2 Matrix{Float64}:\n  0.0310374   0.44579\n  0.0104337   0.218714\n  1.01139     0.710027\n  1.37285     0.954091\n -0.895893   -0.270589\n  0.983361    0.47397\n  1.39855     0.645624\n -1.31861    -0.901482\n  0.0147702   0.241708\n  0.29675     0.483076\n -0.338824   -0.104812\n  0.432442    0.387922\n  0.860418    0.567095\n -0.495306    0.140871\n  0.963084    0.552767\n -1.38901    -1.05926\n  0.469323    0.719196\n  0.0669475   0.512023\n -1.47583    -1.44017\n -1.99789    -3.27656\n\n\n\n\n\n","category":"type"},{"location":"Preprocessing/#LearningHorse.Preprocessing.MinMax","page":"Preprocessing","title":"LearningHorse.Preprocessing.MinMax","text":"MinMax()\n\nMinMax Scaler. This scaler scale data as: tildeboldsymbolx = fracboldsymbolx-min(boldsymbolx)max(boldsymbolx)-min(boldsymbolx)\n\nExample\n\njulia> scaler = MinMax()\nMinMax(Float64[])\n\njulia> fit!(scaler, x)\n2×2 Matrix{Float64}:\n  1.39855   0.954091\n -1.99789  -3.27656\n\njulia> transform!(scaler, x)\n20×2 Matrix{Float64}:\n 0.597368  0.879853\n 0.591301  0.826179\n 0.88601   0.942311\n 0.992432  1.0\n 0.324455  0.710522\n 0.877757  0.886514\n 1.0       0.927088\n 0.199996  0.561398\n 0.592578  0.831614\n 0.675601  0.888666\n 0.488471  0.749707\n 0.715552  0.866175\n 0.841559  0.908526\n 0.442398  0.807779\n 0.871787  0.905139\n 0.179268  0.524104\n 0.72641   0.944478\n 0.607941  0.895508\n 0.153707  0.43407\n 0.0       0.0\n\n\n\n\n\n","category":"type"},{"location":"Preprocessing/#LearningHorse.Preprocessing.Robust","page":"Preprocessing","title":"LearningHorse.Preprocessing.Robust","text":"Robust()\n\nRobust Scaler. This scaler scale data as: tildeboldsymbolx = fracboldsymbolx-Q2Q3 - Q1\n\nExample\n\njulia> scaler = Robust()\nRobust(Float64[])\n\njulia> fit!(scaler, x)\n3×2 Matrix{Float64}:\n 0.412913  0.739911\n 0.602654  0.873014\n 0.849116  0.905986\n\njulia> transform!(scaler, x)\n20×2 Matrix{Float64}:\n -0.0121192   0.041181\n -0.0260262  -0.28201\n  0.649595    0.417263\n  0.89357     0.764633\n -0.637774   -0.978423\n  0.630675    0.0812893\n  0.910919    0.3256\n -0.923097   -1.87636\n -0.0230992  -0.249283\n  0.16723     0.0942497\n -0.261766   -0.742477\n  0.258819   -0.041181\n  0.547691    0.213832\n -0.367388   -0.392802\n  0.616988    0.193438\n -0.970618   -2.10092\n  0.283712    0.430314\n  0.0121192   0.135449\n -1.02922    -2.64305\n -1.38159    -5.25675\n\n\n\n\n\n","category":"type"},{"location":"Preprocessing/#LearningHorse.Preprocessing.fit_transform!","page":"Preprocessing","title":"LearningHorse.Preprocessing.fit_transform!","text":"fit_transform!(scaler, x; dims=1)\n\nfit scaler with x, and transform x.\n\n\n\n\n\n","category":"function"},{"location":"Preprocessing/#LearningHorse.Preprocessing.inv_transform!","page":"Preprocessing","title":"LearningHorse.Preprocessing.inv_transform!","text":"inv_transform!(scaler, x; dims=1)\n\nConvert x in reverse.\n\n\n\n\n\n","category":"function"},{"location":"NeuralNetwork/#NeuralNetwork","page":"NeuralNetwork","title":"NeuralNetwork","text":"","category":"section"},{"location":"NeuralNetwork/#Basics","page":"NeuralNetwork","title":"Basics","text":"","category":"section"},{"location":"NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"To build a neural network with LearningHorse, use the NetWork type.","category":"page"},{"location":"NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"NetWork","category":"page"},{"location":"NeuralNetwork/#LearningHorse.NeuralNetwork.NetWork","page":"NeuralNetwork","title":"LearningHorse.NeuralNetwork.NetWork","text":"NetWork(layers...)\n\nConnect multiple layers, and build a NeuralNetwork. NetWork also supports index. You can also add layers later using the add_layer!() Function.\n\nExample\n\njulia> N = NetWork(Dense(10=>5, relu), Dense(5=>1, relu))\n\njulia> N[1]\n\nDense(IO:10=>5, σ:relu)\n\n\n\n\n\n","category":"type"},{"location":"NeuralNetwork/#Layers","page":"NeuralNetwork","title":"Layers","text":"","category":"section"},{"location":"NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"Conv\n\nDense\n\nDropout\n\nFlatten","category":"page"},{"location":"NeuralNetwork/#LearningHorse.NeuralNetwork.Conv","page":"NeuralNetwork","title":"LearningHorse.NeuralNetwork.Conv","text":"Conv(kernel, in=>out, σ; stride = 1, pading = 0, set_w = \"Xavier\")\n\nThis is the traditional convolution layer. kernel is a tuple of integers that specifies the kernel size, it must have one or two elements. And, in and out specifies number of input and out channels.\n\nThe input data must have a dimensions WHCB(weight, width, channel, batch). If you want to use a data which has a dimentions WHC, you must be add a dimentioins of B.\n\nstride and padding are single integers or tuple(stride is tuple of 2 elements, padding is tuple of 2 elements), and if you specifies KeepSize to padding, we adjust sizes of input and return a matrix which has the same sizes. set_w is Xavier or He, it decide a method to create a first parameter. This parameter is the same as Dense().\n\nExample\n\njulia> C = Conv((2, 2), 2=>2, relu)\nConvolution(k:(2, 2), IO:2 => 2, σ:relu)\n\njulia> C(rand(10, 10, 2, 5)) |> size\n(9, 9, 2, 5)\n\nwarning: Warning\nWhen you specidies same to padding, in some cases, it will be returned one size smaller. Because of its expression.\n\njulia> C = Conv((2, 2), 2=>2, relu, padding = KeepSize)\nConvolution(k:(2, 2), IO:2 => 2, σ:relu\n\njulia> C(rand(10, 10, 2, 5)) |> size\n(9, 9, 2, 5)\n\n\n\n\n\n","category":"type"},{"location":"NeuralNetwork/#LearningHorse.NeuralNetwork.Dense","page":"NeuralNetwork","title":"LearningHorse.NeuralNetwork.Dense","text":"Dense(in=>out, σ; set_w = \"Xavier\", set_b = zeros)\n\nCrate a traditinal Dense layer, whose forward propagation is given by:     y = σ.(W * x .+ b) The input of x should be a Vactor of length in, (Sorry for you can't learn using batch. I'll implement)\n\nExample\n\njulia> D = Dense(5=>2, relu)\nDense(IO:5=>2, σ:relu)\n\njulia> D(rand(Float64, 5)) |> size\n(2,)\n\n\n\n\n\n","category":"type"},{"location":"NeuralNetwork/#LearningHorse.NeuralNetwork.Dropout","page":"NeuralNetwork","title":"LearningHorse.NeuralNetwork.Dropout","text":"Dropout(p)\n\nThis layer dropout the input data.\n\nExample\n\njulia> D = Dropout(0.25) Dropout(0.25)\n\njulia> D(rand(10)) 10-element Array{Float64,1}:  0.0  0.3955865029078952  0.8157710047424143  1.0129613533211907  0.8060508293474877  1.1067504108970596  0.1461289547292684  0.0  0.04581776023870532  1.2794087133638332\n\n\n\n\n\n","category":"type"},{"location":"NeuralNetwork/#LearningHorse.NeuralNetwork.Flatten","page":"NeuralNetwork","title":"LearningHorse.NeuralNetwork.Flatten","text":"Flatten()\n\nThis layer change the dimentions Image to Vector.\n\nExample\n\njulia> F = Flatten()\nFlatten(())\n\njulia> F(rand(10, 10, 2, 5)) |> size\n(1000, )\n\n\n\n\n\n","category":"type"},{"location":"NeuralNetwork/#Optimizers","page":"NeuralNetwork","title":"Optimizers","text":"","category":"section"},{"location":"NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"Descent\n\nMomentum\n\nAdaGrad\n\nAdam","category":"page"},{"location":"NeuralNetwork/#LearningHorse.NeuralNetwork.Descent","page":"NeuralNetwork","title":"LearningHorse.NeuralNetwork.Descent","text":"Descent(η=0.1)\n\nBasic gradient descent optimizer with learning rate η.\n\nParameters\n\nlearning rate : η\n\nExample\n\n\n\n\n\n","category":"type"},{"location":"NeuralNetwork/#LearningHorse.NeuralNetwork.Momentum","page":"NeuralNetwork","title":"LearningHorse.NeuralNetwork.Momentum","text":"Momentum(η=0.01, α=0.9, velocity)\n\nMomentum gradient descent optimizer with learning rate η and parameter of velocity α.\n\nParameters\n\nlearning rate : η\nparameter of velocity : α\n\nExample\n\n\n\n\n\n","category":"type"},{"location":"NeuralNetwork/#LearningHorse.NeuralNetwork.AdaGrad","page":"NeuralNetwork","title":"LearningHorse.NeuralNetwork.AdaGrad","text":"AdaGrad(η = 0.01)\n\nGradient descent optimizer with learning rate attenuation.\n\nParameters\n\nη : initial learning rate\n\nExamples\n\n\n\n\n\n","category":"type"},{"location":"NeuralNetwork/#LearningHorse.NeuralNetwork.Adam","page":"NeuralNetwork","title":"LearningHorse.NeuralNetwork.Adam","text":"Adam(η=0.01, β=(0.9, 0.99))\n\nGradient descent adaptive moment estimation optimizer.\n\nParameters\n\nη : learning rate\nβ : Decay of momentums\n\nExamples\n\n\n\n\n\n","category":"type"},{"location":"Regression/#Regression","page":"Regression","title":"Regression","text":"","category":"section"},{"location":"Regression/#Models","page":"Regression","title":"Models","text":"","category":"section"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Regression.LinearRegression\nRegression.Lasso\nRegression.Ridge","category":"page"},{"location":"Regression/#LearningHorse.Regression.LinearRegression","page":"Regression","title":"LearningHorse.Regression.LinearRegression","text":"LinearRegression()\n\nClassic regression model. This struct has no parameter. If you want to use polynomial model, use Regression.make_design_matrix().\n\nsee also: make_design_matrix\n\nExample\n\njulia> x = [\n    16.862463771320925 68.10823385851712\n    15.382965696961577 65.4313485700859\n    8.916228406218375 53.92034559524475\n    10.560285659132695 59.17305391117168\n    12.142253214135884 62.28708207525656\n    5.362107221163482 43.604947901567414\n    13.893239446341777 62.44348617377496\n    11.871357065173395 60.28433066289655\n    29.83792267802442 69.22281924803998\n    21.327107214235483 70.15810991597944\n    23.852372696012498 69.81780163668844\n    26.269031430914108 67.61037566099782\n    22.78907104644012 67.78105545358633\n    26.73342178134947 68.59263965946904\n    9.107259141706415 56.565383817343495\n    29.38551885863976 68.1005579469209\n    7.935966787763017 53.76264777936664\n    29.01677894379809 68.69484161138638\n    6.839609488194577 49.69794758177567\n    13.95215840314148 62.058116579899085]; #These data are also used to explanations of other functions.\n\njulia> t = [169.80980778351542, 167.9081124078835, 152.30845618985222, 160.3110300206261, 161.96826472170756, 136.02842285615077, 163.98131131382686, 160.117817321485, 172.22758529098235, 172.21342437006865, 171.8939175591617, 169.83018083884602, 171.3878062674257, 170.52487535026015, 156.40282783981309, 170.6488327896672, 151.69267899906185, 172.32478221316322, 145.14365314788827, 163.79383292080666];\n\njulia> model = LinearRegression()\nLinearRegression(Float64[])\n\njulia> fit!(model, x, t)\n3-element Vector{Float64}:\n -0.04772448076255398\n  1.395963968616736\n 76.7817095600793\n\njulia> predict(model, x)\n20-element Vector{Float64}:\n 171.05359766482795\n 167.38737053144575\n 151.62704681535598\n 158.88117658330424\n 163.15274911747872\n 137.3967419011542\n 163.28751869479999\n 160.3699086857777\n 171.99027166957023\n 173.70207799107243\n 173.10650291105486\n 169.9096820022986\n 170.31402414534642\n 171.25872436348817\n 155.31030802635905\n 170.44522606721017\n 151.45368882321284\n 171.29242257091374\n 145.83183688699864\n 162.74674475052848\n\n\n\n\n\n","category":"type"},{"location":"Regression/#LearningHorse.Regression.Lasso","page":"Regression","title":"LearningHorse.Regression.Lasso","text":"Lasso(; alpha = 0.1, tol = 1e-4, mi = 1e+8)\n\nLasso Regression structure. eEach parameters are as follows:\n\nalpha : leaarning rate.\ntol : Allowable error.\nmi : Maximum number of learning.\n\nExample\n\njulia> model = Lasso()\nLasso(Float64[], 0.1, 0.0001, 100000000)\n\njulia> fit!(model, x, t)\n3-element Vector{Float64}:\n   0.0\n   0.5022766549841176\n 154.43624186616267\n\njulia> predict(model, x, t)\n20-element Vector{Float64}:\n 188.64541774549468\n 187.30088075704523\n 181.5191726873298\n 184.15748544986084\n 185.72158909964372\n 176.33798923891868\n 185.80014722707335\n 184.71565381947883\n 189.20524796663838\n 189.67502263476888\n 189.50409373058318\n 188.39535519538825\n 188.481083670683\n 188.88872347085172\n 182.8477136378307\n 188.64156231429416\n 181.43996475587224\n 188.9400571253936\n 179.39836073711297\n 185.6065850765288\n\n\n\n\n\n","category":"type"},{"location":"Regression/#LearningHorse.Regression.Ridge","page":"Regression","title":"LearningHorse.Regression.Ridge","text":"Ridge(alpha = 0.1)\n\nRidge Regression. alpha is the value multiplied by regularization term.\n\nExample\n\njulia> model = Ridge()\nRidge(Float64[], 0.1)\n\njulia> fit!(model, x, t)\n3-element Vector{Float64}:\n -0.5635468573581848\n  2.1185952951687614\n 40.334109796666425\n\njulia> predict(model, x)\n20-element Vector{Float64}:\n 175.12510514593038\n 170.28763505842625\n 149.54478779081344\n 159.7466476176333\n 165.4525001910219\n 129.6935485937018\n 164.79709438985097\n 161.3621431448216\n 170.17418141858434\n 176.95192713546982\n 174.8078461898064\n 168.76930346791391\n 171.09202561187362\n 170.58861763111338\n 155.04089855305028\n 168.05151465675456\n 149.76311329450505\n 169.5183634524783\n 141.7695072903308\n 163.94744858842117\n\n\n\n\n\n","category":"type"},{"location":"Regression/#Other","page":"Regression","title":"Other","text":"","category":"section"},{"location":"Regression/","page":"Regression","title":"Regression","text":"Regression.make_design_matrix","category":"page"},{"location":"Regression/#LearningHorse.Regression.make_design_matrix","page":"Regression","title":"LearningHorse.Regression.make_design_matrix","text":"make_design_matrix(x, dims)\n\nThis function return the design matrix.\n\nExample\n\njulia> make_design_matrix(x, dims = 2) |> size\n(20, 5)\n\n\n\n\n\n","category":"function"},{"location":"Classification/#Classification","page":"Classification","title":"Classification","text":"","category":"section"},{"location":"Classification/#Encoders","page":"Classification","title":"Encoders","text":"","category":"section"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Classification.LabelEncoder\nClassification.OneHotEncoder","category":"page"},{"location":"Classification/#LearningHorse.Classification.LabelEncoder","page":"Classification","title":"LearningHorse.Classification.LabelEncoder","text":"LabelEncoder()\n\nLabelEncoder structure. LE(label; count=false, decode=false) Convert labels(like string) to class numbers(encode), and convert class numbers to labels(decode).\n\nExample\n\njulia> label = [\"Apple\", \"Apple\", \"Pear\", \"Pear\", \"Lemon\", \"Apple\", \"Pear\", \"Lemon\"]\n8-element Vector{String}\n \"Apple\"\n \"Apple\"\n \"Pear\"\n \"Pear\"\n \"Lemon\"\n \"Apple\"\n \"Pear\"\n \"Lemon\"\n\njulia> LE = LabelEncoder()\nLabelEncoder(Dict{Any, Any}())\n\njulia> classes, count = LE(label, count=true) #Encode\n([3.0 3.0 … 1.0 2.0], [3.0 2.0 3.0])\n\njulia> LE(classes, decode=true) #Decode\n1×18 Matrix{String}:\n \"Apple\" \"Apple\" \"Pear\" \"Pear\" \"Lemon\" \"Apple\" \"Pear\" \"Lemon\"\n\n\n\n\n\n","category":"type"},{"location":"Classification/#LearningHorse.Classification.OneHotEncoder","page":"Classification","title":"LearningHorse.Classification.OneHotEncoder","text":"OneHotEncoder()\n\nconvert data to Ont-Hot format. If you specified decode, data will be decoded.\n\nExample\n\njulia> x = [4.9 3.0; 4.6 3.1; 4.4 2.9; 4.8 3.4; 5.1 3.8; 5.4 3.4; 4.8 3.4; 5.2 4.1; 5.5 4.2; 5.5 3.5; 4.8 3.0; 5.1 3.8; 5.0 3.3; 6.4 3.2; 5.7 2.8; 6.1 2.9; 6.7 3.1; 5.6 2.5; 6.3 2.5; 5.6 3.0; 5.6 2.7; 7.6 3.0; 6.4 2.7; 6.4 3.2; 6.5 3.0; 7.7 3.8; 7.2 3.2; 7.2 3.0; 6.3 2.8; 6.1 2.6; 6.3 3.4; 6.0 3.0; 6.9 3.1; 6.7 3.1; 5.8 2.7; 6.8 3.2; 6.3 2.5];#These data are also used to explanations of other functions.\n\njulia> t = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2];\n\njulia> OHE = OneHotEncoder()\nOneHotEncoder()\n\njulia> ct = OHE(t)\n37×3 Matrix{Float64}:\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 0.0 1.0 0.0\n 0.0 1.0 0.0\n 0.0 1.0 0.0\n 0.0 1.0 0.0\n 0.0 1.0 0.0\n ⋮\n 0.0 1.0 0.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n\njulia> OHE(ct, decode = true)\n37-element Vector{Int64}:\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 2\n 2\n 2\n 2\n ⋮\n 2\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n\n\n\n\n\n","category":"type"},{"location":"Classification/#Models","page":"Classification","title":"Models","text":"","category":"section"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Classification.Logistic\nClassification.SVC","category":"page"},{"location":"Classification/#LearningHorse.Classification.Logistic","page":"Classification","title":"LearningHorse.Classification.Logistic","text":"Logistic(; alpha = 0.01, ni = 1000)\n\nLogistic Regression classifier.\n\nThis struct learns classifiers using multi class softmax. Parameter α indicates the learning rate, and ni indicates the number of learnings.\n\nExample\n\njulia> model = Logistic(alpha = 0.1)\nLogistic(0.1, 1000, Matrix{Float64}(undef, 0, 0))\n\njulia> fit!(model, x, ct)\n3×3 Matrix{Float64}:\n  1.80736  1.64037  -0.447735\n -1.27053  1.70026   2.57027\n  4.84966 -0.473835 -1.37582\n\njulia> println(predict(model, x))\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n\n\n\n\n\n","category":"type"},{"location":"Classification/#LearningHorse.Classification.SVC","page":"Classification","title":"LearningHorse.Classification.SVC","text":"SVC(; alpha=0.01, ni=1000)\n\nSupport Vector Machine Classifier.\n\nThis struct learns classifiers using One-Vs-Rest. One-Vs-Rest generates two-class classifiers divided into one class and the other classes using Logistic Regression, adopting the most likely one among all classifiers.\n\nParameter α indicates the learning rate, and ni indicates the number of learnings.\n\nExample\n\njulia> model = SVC()\nSVC(0.01, 1000, Logistic[])\n\njulia> fit!(model, ct)\n3-element Vector{Logistic}:\n Logistic(0.01, 1000, [0.8116709490679518 1.188329050932049; 1.7228257190036231 0.2771742809963788; -0.1519960725403138 2.1519960725403116])\n Logistic(0.01, 1000, [0.9863693439936144 1.0136306560063886; 0.8838433946106077 1.11615660538939; 1.4431044559203794 0.5568955440796174])\n Logistic(0.01, 1000, [1.262510641510418 0.7374893584895849; 0.5242383002319192 1.4757616997680822; 1.864635796779504 0.135364203220495])\n\njulia> println(predict(model, x))\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n\n\n\n\n\n","category":"type"},{"location":"Classification/#DecisionTree","page":"Classification","title":"DecisionTree","text":"","category":"section"},{"location":"Classification/#Models-2","page":"Classification","title":"Models","text":"","category":"section"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Tree.DecisionTree\nTree.RandomForest","category":"page"},{"location":"Classification/#LearningHorse.Tree.DecisionTree","page":"Classification","title":"LearningHorse.Tree.DecisionTree","text":"DecisionTree(; alpha = 0.01)\n\nNormal DecisionTree. alpha specify the complexity of the model. If it's small, it's complicated, and if it's big, it's simple.\n\nExample\n\njulia> tree = DecisionTree()\nDecisionTree(0.01, Dict{Any, Any}(), Any[])\n\njulia> fit!(tree, x, t)\nDict{String, Any} with 5 entries:\n  \"left\"        => Dict{String, Any}(\"left\"=>Dict{String, Union{Nothing, Vector…\n  \"class_count\" => [8, 13, 16]\n  \"threshold\"   => 5.7\n  \"right\"       => Dict{String, Any}(\"left\"=>Dict{String, Union{Nothing, Vector…\n  \"feature_id\"  => 1\n\njulia> println(predict(tree, x))\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n\n\n\n\n\n","category":"type"},{"location":"Classification/#LearningHorse.Tree.RandomForest","page":"Classification","title":"LearningHorse.Tree.RandomForest","text":"RandomForest(nt; alpha = 0.01)\n\nRandomForest Model. nt is the number of trees, and alpha is the same as alpha in DecisionTree.\n\nExample\n\njulia> model = RandomForest(10)\nRandomForest(0.01, 10, DecisionTree[], Vector{Any}[], #undef)\n\njulia> fit!(model, x, t)\n10×1 Matrix{Int64}:\n 1\n 2\n 2\n 2\n 2\n 1\n 1\n 1\n 1\n 1\n\njulia> println(predict(model, x))\nAny[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n\n\n\n\n\n","category":"type"},{"location":"Classification/#VisualTool","page":"Classification","title":"VisualTool","text":"","category":"section"},{"location":"Classification/","page":"Classification","title":"Classification","text":"Tree.MV","category":"page"},{"location":"Classification/#LearningHorse.Tree.MV","page":"Classification","title":"LearningHorse.Tree.MV","text":"MV(path, forest; rounded=false, bg=\"#ffffff\", fc=\"#000000\", label=\"Tree\", fs=\"18\")\n\nMake DicisionTree and RandomForest Visual(make a dot file, see also Graphviz). The arguments are as follows:\n\npath : The full path of the dot file. The suffix must be .dot.\nforest : The model.\nrounded : If rounded is true, the nodes will be rounded.\nbg : Background color, type of this must be String.\nfc : Font color, type of this must be String.\nlabel : The label of the graph.\nfs : Font size, type of this must be String.\n\nExample\n\njulia> MV(\"/home/ubuntu/test.dot\", model, rounded = true)\n\n\n\n\n\n","category":"function"},{"location":"Classification/","page":"Classification","title":"Classification","text":"If you make the model created in DecisionTree Example visualized, it'll be like this: (Image: Tree Visualized)","category":"page"},{"location":"#LearningHorse.jl","page":"Home","title":"LearningHorse.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LearningHorse provides an easy-to-use machine learning library","category":"page"},{"location":"#Resources","page":"Home","title":"Resources","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation\nSource code","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LearningHorse can be installed using the Julia package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and run.","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add LearningHorse","category":"page"},{"location":"Japanese/#LearningHorse.jl-日本語版マニュアル","page":"日本語版マニュアル","title":"LearningHorse.jl 日本語版マニュアル","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"LearningHorseHorseは使いやすく、高速な機械学習ライブラリを提供します。回帰のような単純なアルゴリズムから、ニューラルネットワークのような高度なモデルまで、Juliaで機械学習を学び始めたい方にうってつけです！","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"warning: Warning\nこの日本語版ドキュメントは古いバージョンのドキュメントしか乗っておらず、ほぼ使えません！申し訳ありませんが、しばらく英語版ドキュメントで仕様を確認してください","category":"page"},{"location":"Japanese/#インストール","page":"日本語版マニュアル","title":"インストール","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"LearningHorse.jlはREPLで]を押してパッケージモードに入って、以下のようにしてインストールできます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"pkg > add LearningHorse","category":"page"},{"location":"Japanese/#使い方","page":"日本語版マニュアル","title":"使い方","text":"","category":"section"},{"location":"Japanese/#回帰の関数","page":"日本語版マニュアル","title":"回帰の関数","text":"","category":"section"},{"location":"Japanese/#SGDの使い方","page":"日本語版マニュアル","title":"SGDの使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"勾配最急下法（SGD）は以下のようにして使うことができます","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\nx = [15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.1 26.95 5.68 21.76]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.71 173.29 159.31 171.52 138.96 165.87]\nw = Linear_Regression.SGD.fit(x, t)\nprintln(w) #[1.539947 136.176160] 一つ目の要素が傾き、二つ目の要素が切片を表す。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数は次の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.SGD.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列（ただし一次元）\n- t：目的変数の行列","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"以下キーワード引数 \t- alpha：学習率、デフォルト値は0.001 \t- tau_max：繰り返しの最大回数、デフォルト値は100000 \t- eps：繰り返しをやめる勾配の絶対値の閾値、デフォルト値は0.1","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"返り値は、Array型で返されます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"構築したモデルを使った予測は以下の関数で行えます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.SGD.predict()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数\n- w：フィットして得られたwのArray","category":"page"},{"location":"Japanese/#重回帰の使い方","page":"日本語版マニュアル","title":"重回帰の使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"重回帰は以下のようにして使うことができます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\nx =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\nw = Linear_Regression.MR.fit(x, t) #ここでは、逆行列が存在しないため、Errorが出る\nprintln(w)","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"重回帰では、プログラム内で逆行列を生成できなかった場合、以下のようなErrorを返します。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Perhaps the matrix x you passed does not have an inverse matrix. In that case, use ridge regression.","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"エラー文の通りリッジ回帰を使うのも一つの手段ではありますが、いずれにしても固有値が0に近いことは事実であるが故に、不安定なモデルになってしまうため、相関の高いデータを一列残して削除することで解決する等の手段も考ると良いです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.MR.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列\n- t：目的変数の行列","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"返り値はArray型で返され、一つ目の要素がモデルの切片、それ以降はそれぞれの従属変数の係数となっています。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"構築したモデルを使った予測は以下の関数で行えます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.MR.predict()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列\n- w：フィットして得られたwのArray","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"返り値はArray型で返されます。","category":"page"},{"location":"Japanese/#リッジ回帰の使い方","page":"日本語版マニュアル","title":"リッジ回帰の使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"リッジ回帰は以下のようにして使うことができます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\nx =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\nw = Linear_Regression.RR.fit(x, t)\nprintln(w) #[65.47458454192515; 0.1042034860770639; 1.5756199878279644] 一つ目の要素が切片、二つ以降の要素は従属変数xの係数を表す","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.RR.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列\n- t：目的変数の行列","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"キーワード引数 \t- alpha：学習率、デフォルト値は0.1 戻り値はArray型で返され、一つ目の要素がモデルの切片、二つ目以降の要素は従属変数xの係数を表します。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"構築したモデルを使った予測は以下の関数で行えます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.RR.predict()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列\n- w：フィットして得たwのArray","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"返り値はArray型で返されます。","category":"page"},{"location":"Japanese/#ラッソ回帰の使い方","page":"日本語版マニュアル","title":"ラッソ回帰の使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"ラッソ回帰は以下のようにして使うことができます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\nx =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\nw = Linear_Regression.LR.fit(x, t)\nprintln(w) # [144.61846209854855, 0.951158307335192, 0.0] 一つ目の要素が切片、二つ目以降が従属変数xの係数を表す","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.LR.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列\n- t：目的変数の行列","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"以下キーワード引数 \t- alpha：学習率、デフォルト値は0.1 \t- tol：繰り返しをやめる誤差の絶対値の闘値、デフォルト値は0.0001 \t- mi：繰り返しの最大回数、デフォルト値は1000000 戻り値はArray型で返され、一つ目の要素はモデルの切片、二つ目以降が従属変数xの係数を表します。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"構築したモデルを使った予測は以下の関数で行えます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Linear_Regression.LR.predict()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数\n- w：フィットして得られたwのArray","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"返り値はLinearAlgebra.Adjoint型で返されます。","category":"page"},{"location":"Japanese/#基底関数モデルの使い方","page":"日本語版マニュアル","title":"基底関数モデルの使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"基底関数モデルは以下のようにして使うことができます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\n x =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\n\n#ガウス基底関数を使用\nw, m, s = LinearRegression.BFM.fit(x[1, :], t, 4)\nprintln(\"w:\", w, \"m:\", m, \"s:\", s)\n#w:[-10.6733; 65.4475; -17.7623; 66.6557; 104.918]m:Any[5, 12.3333, 19.6667, 27.0]s:7.333333333333333\n\np = LinearRegression.BFM.predict(x[1, :], w, m, s)\nprintln(p)\n#[165.07, 168.55, 132.278, 162.546, 150.433, 144.04, 154.507, 164.011, 164.902, 165.417, 165.082, 167.712, 156.153, 169.583, 135.749, 167.366]\n\n#多項式基底関数を使用\nw, m, s = LinearRegression.BFM.fit(x[1, :], t, 4, alpha = 0.1, f = \"polynomial\")\nprintln(\"w, m, s(polynomial):\", w, m, s)\n#w, m, s(polynomial):[30.074721512969795; 17.649472753936088; -0.9665443355191238; 0.01733639801805678; 30.074721512971067]Any[5, 12.333333333333332, 19.666666666666664, 26.999999999999996]7.333333333333333\n\np = LinearRegression.BFM.predict(x[1, :], w, m, s, f = \"polynomial\")\nprintln(\"p(polynomial):\", p)\n#p(polynomial):[166.04905485192677, 165.72435351634755, 126.40024815990033, 163.70105505929791, 151.81467689141562, 144.29063632133014, 156.07721189214902, 165.05792844782184, 165.89978336872304, 165.6418324683704, 166.05887282906252, 165.26960817342376,157.70790163159944, 173.13926201582598, 132.39231072417851, 165.16779149696566]","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"関数は、predictとfitのみですが、キーワード引数 f を指定することで、多項式回帰とガウス基底関数モデルの二つのモデルを使うことができます。 引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"学習する関数","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"LinearRegression.BFM.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数のベクトル（行列を入れることはできません）\n- t：目的変数のベクトル（行列を入れることはできません）\n- M：フィットする次元数、2次元ならM = 2とします。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"以下、キーワード引数 \t- m：ガウス基底関数の場合に指定することができます。デフォルトでは各ガウス関数の中心のベクトルを使用します \t- s：mと同様、ガウス関数の場合に指定することができます。デフォルトでは各ガウス関数の間隔を表すスカラーを使用します \t- alpha：計画行列の計算を行う際に加算される正則化項にかけられる係数です。デフォルト値は0.0 \t- f：基底関数の種類の指定、ガウス関数は文字列\"gauss\"、多項式回帰の場合は\"polynomial\"を渡します。デフォルト値は\"gauss\"であることに注意してください","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"予測する関数","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"LinearRegression.BFM.predict()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：予測に使用する従属変数のベクトル\n- w：フィットして得られたモデルのArrayのwを渡す","category":"page"},{"location":"Japanese/#分類の関数","page":"日本語版マニュアル","title":"分類の関数","text":"","category":"section"},{"location":"Japanese/#ラベルエンコーダーの使い方","page":"日本語版マニュアル","title":"ラベルエンコーダーの使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"ラベルエンコーダーは以下のように使います。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\nlabel = [\"Apple\", \"Apple\", \"Pear\", \"Pear\", \"Lemon\", \"Apple\", \"Pear\", \"Lemon\"]\nl, d = Classification.LE(label)\nprintln(l)\n#[3.0 3.0 1.0 1.0 2.0 3.0 1.0 2.0]\nprintln(\"d:\", d)\n#d:Dict{Any, Any}(\"Pear\" => 1, \"Lemon\" => 2, \"Apple\" => 3)","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"ラベルエンコーダーはSet型を使用して実装されているため要素の順番は保証されません、そのため、どのクラスがどのラベルに対応するのかを示すDictが返されます。引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Classification.LE()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- t：変換するラベルのArray","category":"page"},{"location":"Japanese/#One-Hot形式への変換","page":"日本語版マニュアル","title":"One Hot形式への変換","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"One Hot形式への変換を行うCTOH関数は以下のようにして使います。（ただしこのコードは先ほどのコードの続きとします。）","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"t = Classification.CTOH(l)\nprintln(\"t:\", t)\n#t:[0.0 0.0 1.0; 0.0 0.0 1.0; 1.0 0.0 0.0; 1.0 0.0 0.0; 0.0 1.0 0.0; 0.0 0.0 1.0; 1.0 0.0 0.0; 0.0 1.0 0.0]","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"この関数は渡されるベクトルのクラスの表し方が、0から始まるものは変換可能ですが、クラスを負の数で表したベクトルを渡すとエラーとなリます。 引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Classification.CTOH()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- t：変換するクラスを数値で表したベクトル。","category":"page"},{"location":"Japanese/#マルチクラスソフトマックスを使用したマルチクラス分類","page":"日本語版マニュアル","title":"マルチクラスソフトマックスを使用したマルチクラス分類","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"MS関数を使用したマルチクラスソフトマックスを使用した分類は以下のように行います。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"x = [5.1 3.5; 4.9 3.0; 4.7 3.2; 4.6 3.1; 5.0 3.6; 5.4 3.9; 4.6 3.4; 5.0 3.4; 4.4 2.9; 4.9 3.1; 5.4 3.7; 4.8 3.4; 4.8 3.0; 4.3 3.0; 5.8 4.0; 5.7 4.4; 5.4 3.9; 5.1 3.5; 5.7 3.8; 5.1 3.8; 5.4 3.4; 5.1 3.7; 4.6 3.6; 5.1 3.3; 4.8 3.4; 5.0 3.0; 5.0 3.4; 5.2 3.5; 5.2 3.4; 4.7 3.2; 4.8 3.1; 5.4 3.4; 5.2 4.1; 5.5 4.2; 4.9 3.1; 5.0 3.2; 5.5 3.5; 4.9 3.6; 4.4 3.0; 5.1 3.4; 5.0 3.5; 4.5 2.3; 4.4 3.2; 5.0 3.5; 5.1 3.8; 4.8 3.0; 5.1 3.8; 4.6 3.2; 5.3 3.7; 5.0 3.3; 7.0 3.2; 6.4 3.2; 6.9 3.1; 5.5 2.3; 6.5 2.8; 5.7 2.8; 6.3 3.3; 4.9 2.4; 6.6 2.9; 5.2 2.7; 5.0 2.0; 5.9 3.0; 6.0 2.2; 6.1 2.9; 5.6 2.9; 6.7 3.1; 5.6 3.0; 5.8 2.7; 6.2 2.2; 5.6 2.5; 5.9 3.2; 6.1 2.8; 6.3 2.5; 6.1 2.8; 6.4 2.9; 6.6 3.0; 6.8 2.8; 6.7 3.0; 6.0 2.9; 5.7 2.6; 5.5 2.4; 5.5 2.4; 5.8 2.7; 6.0 2.7; 5.4 3.0; 6.0 3.4; 6.7 3.1; 6.3 2.3; 5.6 3.0; 5.5 2.5; 5.5 2.6; 6.1 3.0; 5.8 2.6; 5.0 2.3; 5.6 2.7; 5.7 3.0; 5.7 2.9; 6.2 2.9; 5.1 2.5; 5.7 2.8; 6.3 3.3; 5.8 2.7; 7.1 3.0; 6.3 2.9; 6.5 3.0; 7.6 3.0; 4.9 2.5; 7.3 2.9; 6.7 2.5; 7.2 3.6; 6.5 3.2; 6.4 2.7; 6.8 3.0; 5.7 2.5; 5.8 2.8; 6.4 3.2; 6.5 3.0; 7.7 3.8; 7.7 2.6; 6.0 2.2; 6.9 3.2; 5.6 2.8; 7.7 2.8; 6.3 2.7; 6.7 3.3; 7.2 3.2; 6.2 2.8; 6.1 3.0; 6.4 2.8; 7.2 3.0; 7.4 2.8; 7.9 3.8; 6.4 2.8; 6.3 2.8; 6.1 2.6; 7.7 3.0; 6.3 3.4; 6.4 3.1; 6.0 3.0; 6.9 3.1; 6.7 3.1; 6.9 3.1; 5.8 2.7; 6.8 3.2; 6.7 3.3; 6.7 3.0; 6.3 2.5; 6.5 3.0; 6.2 3.4; 5.9 3.0]\nt = [0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2]\nt = Classification.CTOH(t)\nw = Classification.MS.fit(x, t, alpha = 0.1)\nprintln(w)\n#[1.7209083498295872 1.8953943405297293 -0.6163026903593205; -1.256172465412238 1.8287641755675748 2.4274082898446574; 4.812985176776389 -0.7076002068908029 -1.1053849698855869]\np = Classification.MS.predict(x, w)\nprintln(p)\n#[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 1, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2]","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"学習を行う関数","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Classification.MS.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列（Array）\n- t：LEやCTOHを使用して整形したクラスの目的変数の行列（Array）","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"以下はキーワード引数 \t- alpha：最適なパラメータを勾配最急下法で探索する際の学習率、デフォルト値は0.01 \t- tau_max：alphaと同じくパラメータを最適化する際の最大繰り返し回数、デフォルト値は1000","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"予測を行う関数","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Classification.MS.predict()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：予測に使用する従属変数の行列\n- w：フィットして得られたパラメータwのArray","category":"page"},{"location":"Japanese/#One-vs-Restを使用したマルチクラス分類","page":"日本語版マニュアル","title":"One-vs-Restを使用したマルチクラス分類","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"One-vs-Restは以下のようにして使用することができます。（このコードは先ほどのマルチクラスソフトマックスの続きとします。）","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"w = Classification.OVR.fit(x, t; alpha = 0.1)\nprintln(\"w:\", w)\n#w:Any[[1.4720211645121106 -3.049021804351106 4.768579857703322], [1.496138559080458 0.5690372319250491 -1.8729783593510938], [-1.2013066350855492 1.258936673691226 -2.3333697507254283]]\np = Classification.OVR.predict(x, w)\nprintln(\"predict:\", p)\n#predict:Any[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 1, 1, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3,3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2]","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"学習を行う関数","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Classification.OVR.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列（Array）\n- t：LEやCTOHを使用して整形したクラスの目的変数の行列（Array）","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"以下はキーワード引数 \t- alpha：最適なパラメータを勾配最急下法で探索する際の学習率、デフォルト値は0.01 \t- tau_max：alphaと同じくパラメータを最適化する際の最大繰り返し回数、デフォルト値は1000","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"予測を行う関数","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Classification.OVR.predict()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：予測に使用する従属変数の行列\n- w：fitで得られたパラメータwのArray","category":"page"},{"location":"Japanese/#損失関数","page":"日本語版マニュアル","title":"損失関数","text":"","category":"section"},{"location":"Japanese/#MSE（平均二乗誤差）の使い方","page":"日本語版マニュアル","title":"MSE（平均二乗誤差）の使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"平均二乗誤差は以下のようにして使うことができます。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\nmean = Loss_Function.MSE([1, 2, 3, 4], [1, 2, 3, 4], [1, 2])\nprintln(mean) #4\n\nx = [15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76]\nt = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]\n\n#ガウス基底関数の平均二乗誤差\nw, m, s = LinearRegression.BFM.fit(x[1, :], t, 4)\nmse = LossFunction.MSE(x[1, :], t, w, b = \"gauss\", m = m, s = s)\nprintln(\"mse(gauss):\", mse)\n#mse(gauss):16.777305554499097\n\n#多項式回帰の平均二乗誤差\nw, m, s = LinearRegression.BFM.fit(x[1, :], t, 4, alpha = 0.1, f = \"polynomial\")\nmse = LossFunction.MSE(x[1, :], t, w, b = \"polynomial\", m = m, s = s)\nprintln(\"mse(polynomial):\", mse)\n#mse(polynomial):18.491924125591847","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数bを指定することによって、基底関数モデルのMSEの算出が可能です。 引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"LossFunction.MSE()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列\n- t：目的変数の行列\n- w：フィットして得られたモデルのパラメータ","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"以下キーワード引数 \t- b：基底関数のMSEを算出する際に指定する。ガウス基底関数なら\"gauss\"、多項式回帰なら\"polynomial\" \t- m：ガウス基底関数のMSEを算出する際に指定できる。デフォルトでは各ガウス関数の中心のベクトルを使用して計算 \t- s：ガウス関数のMSEを算出する際に指定できる。デフォルトでは各ガウス関数の間隔を使用して計算 \t- mean_f：二乗して足し合わせた誤差の総和を割って、平均二乗誤差にするかどうか、デフォルト値はtrue","category":"page"},{"location":"Japanese/#CEE（交差エントロピー誤差）の使い方","page":"日本語版マニュアル","title":"CEE（交差エントロピー誤差）の使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"交差エントロピー誤差は以下のようにして使います。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"using LearningHorse\n#MSのCEE\nx = [5.1 3.5; 4.9 3.0; 4.7 3.2; 4.6 3.1; 5.0 3.6; 5.4 3.9; 4.6 3.4; 5.0 3.4; 4.4 2.9; 4.9 3.1; 5.4 3.7; 4.8 3.4; 4.8 3.0; 4.3 3.0; 5.8 4.0; 5.7 4.4; 5.4 3.9; 5.1 3.5; 5.7 3.8; 5.1 3.8; 5.4 3.4; 5.1 3.7; 4.6 3.6; 5.1 3.3; 4.8 3.4; 5.0 3.0; 5.0 3.4; 5.2 3.5; 5.2 3.4; 4.7 3.2; 4.8 3.1; 5.4 3.4; 5.2 4.1; 5.5 4.2; 4.9 3.1; 5.0 3.2; 5.5 3.5; 4.9 3.6; 4.4 3.0; 5.1 3.4; 5.0 3.5; 4.5 2.3; 4.4 3.2; 5.0 3.5; 5.1 3.8; 4.8 3.0; 5.1 3.8; 4.6 3.2; 5.3 3.7; 5.0 3.3; 7.0 3.2; 6.4 3.2; 6.9 3.1; 5.5 2.3; 6.5 2.8; 5.7 2.8; 6.3 3.3; 4.9 2.4; 6.6 2.9; 5.2 2.7; 5.0 2.0; 5.9 3.0; 6.0 2.2; 6.1 2.9; 5.6 2.9; 6.7 3.1; 5.6 3.0; 5.8 2.7; 6.2 2.2; 5.6 2.5; 5.9 3.2; 6.1 2.8; 6.3 2.5; 6.1 2.8; 6.4 2.9; 6.6 3.0; 6.8 2.8; 6.7 3.0; 6.0 2.9; 5.7 2.6; 5.5 2.4; 5.5 2.4; 5.8 2.7; 6.0 2.7; 5.4 3.0; 6.0 3.4; 6.7 3.1; 6.3 2.3; 5.6 3.0; 5.5 2.5; 5.5 2.6; 6.1 3.0; 5.8 2.6; 5.0 2.3; 5.6 2.7; 5.7 3.0; 5.7 2.9; 6.2 2.9; 5.1 2.5; 5.7 2.8; 6.3 3.3; 5.8 2.7; 7.1 3.0; 6.3 2.9; 6.5 3.0; 7.6 3.0; 4.9 2.5; 7.3 2.9; 6.7 2.5; 7.2 3.6; 6.5 3.2; 6.4 2.7; 6.8 3.0; 5.7 2.5; 5.8 2.8; 6.4 3.2; 6.5 3.0; 7.7 3.8; 7.7 2.6; 6.0 2.2; 6.9 3.2; 5.6 2.8; 7.7 2.8; 6.3 2.7; 6.7 3.3; 7.2 3.2; 6.2 2.8; 6.1 3.0; 6.4 2.8; 7.2 3.0; 7.4 2.8; 7.9 3.8; 6.4 2.8; 6.3 2.8; 6.1 2.6; 7.7 3.0; 6.3 3.4; 6.4 3.1; 6.0 3.0; 6.9 3.1; 6.7 3.1; 6.9 3.1; 5.8 2.7; 6.8 3.2; 6.7 3.3; 6.7 3.0; 6.3 2.5; 6.5 3.0; 6.2 3.4; 5.9 3.0]\nt = [0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2]\nt = Classification.CTOH(t)\nw = Classification.MS.fit(x, t, alpha = 0.1)\ncee = LossFunction.CEE(x, t, w, t_f = true)\nprintln(\"cee:\", cee)\n#52.20173706758349\n\n#OVRのCEE\nw = Classification.OVR.fit(x, t; alpha = 0.1)\ncee = LossFunction.CEE(x, t, w, sigmoid_f = true)\nprintln(\"cee:\", cee)\n#28.702950289767305","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"LossFunction.CEE()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：従属変数の行列\n- t：目的変数の行列\n- w：フィットして得られたモデルのパラメータ","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"以下キーワード引数 \t- meanf：誤差の和を割って平均交差エントロピー誤差にするかどうか、デフォルトではtrue \t- sigmoidf：OVRでCEEを求める場合は、この引数をtrueにする必要があります。デフォルト値はfalse \t- t_f：CEEの求める式を指定します。trueなら（式−1）に、falseなら（式-2）になります。デフォルト値はfalse","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"（Nは要素の数、Kは入力の次元数） （式-1）-frac1Nsum_n=0^N-1 t_nlogy_n+(1-t_n)log(1-y_n)","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"（式-2）-frac1Nsum_n=0^N-1sum_k=0^K-1t_nklogy_nk","category":"page"},{"location":"Japanese/#Preprocessing","page":"日本語版マニュアル","title":"Preprocessing","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Preprocessingの関数は、全て引数や仕様が同じなので、Standard Scalerのみ書きます。","category":"page"},{"location":"Japanese/#Standard-Scalerの使い方","page":"日本語版マニュアル","title":"Standard Scalerの使い方","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"StandardScaler（及びその他のPreprocessingの関数）は以下のように使います。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"x = [1 2 3 4 5; 2 3 4 5 6]]\nsd = Preprocessing.SS.fit_transform(x, axis = 2)\nprintln(\"sd:\", sd)\n#sd:(Any[[-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518], [-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518]], Any[[3.0, 1.5811388300841898], [4.0, 1.5811388300841898]])\ninv = Preprocessing.SS.inverse_transform(sd[1], sd[2], axis = 2)\nprintln(\"inv:\", inv)\n#inv:Any[[1.0 2.0 3.0 4.0 5.0], [2.0 3.0 4.0 5.0 6.0]]\np = Preprocessing.SS.fit(x, axis = 2)\nprintln(\"fit:\", p)\n#fit:Any[[3.0, 1.5811388300841898], [4.0, 1.5811388300841898]]\np = Preprocessing.SS.transform(x, p, axis = 2)\nprintln(\"p:\", p)\n#p:Any[[-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518], [-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518]]","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"fit関数を使ったのち、transform関数を使うのと、fittransform関数を使うのは完全に等価です。 fit関数は、スケーリングに必要な数値を算出し返すだけの関数で、fitで得られた数値を使ってスケーリングするのがtransform関数です。fittransform関数はこれらの処理を一度に行います。 一方、invers_transform関数は、スケーリングした数値やモデルで予測した値を元の値に戻す関数です。 引数は以下の通りです。","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Preprocessing.SS.fit_transform()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：スケーリングする行列\n- axis：スケーリングする軸、各特徴量が各列に並んでいればaxis = 1、各行に並んでいればaxis = 2を指定する。デフォルト値は1","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Preprocessing.SS.fit()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：スケーリングする行列\n-axis：スケーリングする軸、各特徴量が各列に並んでいればaxis = 1、各行に並んでいればaxis = 2を指定する。デフォルト値は1","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Preprocessing.SS.transform()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：スケーリングする行列\n- p：fit関数やfit_transform関数で得られた数値pを渡す\n- axis：スケーリングする軸、各特徴量が各列に並んでいればaxis = 1、各行に並んでいればaxis = 2を指定する。デフォルト値は1","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"Preprocessing.SS.invers_transform()","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"- x：逆にスケーリングする行列\n- p：スケーリングする際に求められた数値pを渡す\n- axis：逆にスケーリングする軸、各特徴量が各列に並んでいればaxis = 1、各行に並んでいればaxis = 2を指定する。デフォルト値は1","category":"page"},{"location":"Japanese/#その他の関数","page":"日本語版マニュアル","title":"その他の関数","text":"","category":"section"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"MinMaxScalerはPreprocessing.MMで使うことができます。 MMは以下のような式に基づいてスケーリングします。 tildeboldsymbolx = fracboldsymbolx-min(boldsymbolx)max(boldsymbolx)-min(boldsymbolx)","category":"page"},{"location":"Japanese/","page":"日本語版マニュアル","title":"日本語版マニュアル","text":"RobustScalerはPreprocessing.RSで使うことができます。 RSは以下のような式に基づいてスケーリングします。 tildeboldsymbolx = fracboldsymbolx-Q2Q3 - Q1","category":"page"}]
}
