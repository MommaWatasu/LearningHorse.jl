<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>日本語版マニュアル · LearningHorse</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="LearningHorse logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">LearningHorse</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../Regression/">Regression</a></li><li><a class="tocitem" href="../Classification/">Classification</a></li><li><a class="tocitem" href="../Preprocessing/">Preprocessing</a></li><li><a class="tocitem" href="../LossFunction/">LossFunction</a></li><li><a class="tocitem" href="../NeuralNetwork/">NeuralNetwork</a></li></ul></li><li class="is-active"><a class="tocitem" href>日本語版マニュアル</a><ul class="internal"><li><a class="tocitem" href="#インストール"><span>インストール</span></a></li><li><a class="tocitem" href="#使い方"><span>使い方</span></a></li><li><a class="tocitem" href="#回帰の関数"><span>回帰の関数</span></a></li><li><a class="tocitem" href="#分類の関数"><span>分類の関数</span></a></li><li><a class="tocitem" href="#損失関数"><span>損失関数</span></a></li><li><a class="tocitem" href="#Preprocessing"><span>Preprocessing</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>日本語版マニュアル</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>日本語版マニュアル</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/QGMW22/LearningHorse.jl/blob/master/docs/src/Japanese.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="LearningHorse.jl-日本語版マニュアル"><a class="docs-heading-anchor" href="#LearningHorse.jl-日本語版マニュアル">LearningHorse.jl 日本語版マニュアル</a><a id="LearningHorse.jl-日本語版マニュアル-1"></a><a class="docs-heading-anchor-permalink" href="#LearningHorse.jl-日本語版マニュアル" title="Permalink"></a></h1><p>LearningHorseHorseは使いやすく、高速な機械学習ライブラリを提供します。回帰のような単純なアルゴリズムから、ニューラルネットワークのような高度なモデルまで、Juliaで機械学習を学び始めたい方にうってつけです！</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>この日本語版ドキュメントは古いバージョンのドキュメントしか乗っておらず、ほぼ使えません！申し訳ありませんが、しばらく英語版ドキュメントで仕様を確認してください</p></div></div><h2 id="インストール"><a class="docs-heading-anchor" href="#インストール">インストール</a><a id="インストール-1"></a><a class="docs-heading-anchor-permalink" href="#インストール" title="Permalink"></a></h2><p>LearningHorse.jlはREPLで<code>]</code>を押してパッケージモードに入って、以下のようにしてインストールできます。</p><pre><code class="nohighlight hljs">pkg &gt; add LearningHorse</code></pre><h2 id="使い方"><a class="docs-heading-anchor" href="#使い方">使い方</a><a id="使い方-1"></a><a class="docs-heading-anchor-permalink" href="#使い方" title="Permalink"></a></h2><h2 id="回帰の関数"><a class="docs-heading-anchor" href="#回帰の関数">回帰の関数</a><a id="回帰の関数-1"></a><a class="docs-heading-anchor-permalink" href="#回帰の関数" title="Permalink"></a></h2><h3 id="SGDの使い方"><a class="docs-heading-anchor" href="#SGDの使い方">SGDの使い方</a><a id="SGDの使い方-1"></a><a class="docs-heading-anchor-permalink" href="#SGDの使い方" title="Permalink"></a></h3><p>勾配最急下法（SGD）は以下のようにして使うことができます</p><pre><code class="nohighlight hljs">using LearningHorse
x = [15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.1 26.95 5.68 21.76]
t = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.71 173.29 159.31 171.52 138.96 165.87]
w = Linear_Regression.SGD.fit(x, t)
println(w) #[1.539947 136.176160] 一つ目の要素が傾き、二つ目の要素が切片を表す。</code></pre><p>引数は次の通りです。</p><ul><li>Linear_Regression.SGD.fit()</li></ul><pre><code class="nohighlight hljs">- x：従属変数の行列（ただし一次元）
- t：目的変数の行列</code></pre><p>以下キーワード引数 	- alpha：学習率、デフォルト値は0.001 	- tau_max：繰り返しの最大回数、デフォルト値は100000 	- eps：繰り返しをやめる勾配の絶対値の閾値、デフォルト値は0.1</p><p>返り値は、Array型で返されます。</p><p>構築したモデルを使った予測は以下の関数で行えます。</p><ul><li>Linear_Regression.SGD.predict()</li></ul><pre><code class="nohighlight hljs">- x：従属変数
- w：フィットして得られたwのArray</code></pre><h3 id="重回帰の使い方"><a class="docs-heading-anchor" href="#重回帰の使い方">重回帰の使い方</a><a id="重回帰の使い方-1"></a><a class="docs-heading-anchor-permalink" href="#重回帰の使い方" title="Permalink"></a></h3><p>重回帰は以下のようにして使うことができます。</p><pre><code class="nohighlight hljs">using LearningHorse
x =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]
t = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]
w = Linear_Regression.MR.fit(x, t) #ここでは、逆行列が存在しないため、Errorが出る
println(w)</code></pre><p>重回帰では、プログラム内で逆行列を生成できなかった場合、以下のようなErrorを返します。</p><pre><code class="nohighlight hljs">Perhaps the matrix x you passed does not have an inverse matrix. In that case, use ridge regression.</code></pre><p>エラー文の通りリッジ回帰を使うのも一つの手段ではありますが、いずれにしても固有値が0に近いことは事実であるが故に、不安定なモデルになってしまうため、相関の高いデータを一列残して削除することで解決する等の手段も考ると良いです。</p><p>引数は以下の通りです。</p><ul><li>Linear_Regression.MR.fit()</li></ul><pre><code class="nohighlight hljs">- x：従属変数の行列
- t：目的変数の行列</code></pre><p>返り値はArray型で返され、一つ目の要素がモデルの切片、それ以降はそれぞれの従属変数の係数となっています。</p><p>構築したモデルを使った予測は以下の関数で行えます。</p><ul><li>Linear_Regression.MR.predict()</li></ul><pre><code class="nohighlight hljs">- x：従属変数の行列
- w：フィットして得られたwのArray</code></pre><p>返り値はArray型で返されます。</p><h3 id="リッジ回帰の使い方"><a class="docs-heading-anchor" href="#リッジ回帰の使い方">リッジ回帰の使い方</a><a id="リッジ回帰の使い方-1"></a><a class="docs-heading-anchor-permalink" href="#リッジ回帰の使い方" title="Permalink"></a></h3><p>リッジ回帰は以下のようにして使うことができます。</p><pre><code class="nohighlight hljs">using LearningHorse
x =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]
t = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]
w = Linear_Regression.RR.fit(x, t)
println(w) #[65.47458454192515; 0.1042034860770639; 1.5756199878279644] 一つ目の要素が切片、二つ以降の要素は従属変数xの係数を表す</code></pre><p>引数は以下の通りです。</p><ul><li>Linear_Regression.RR.fit()</li></ul><pre><code class="nohighlight hljs">- x：従属変数の行列
- t：目的変数の行列</code></pre><p>キーワード引数 	- alpha：学習率、デフォルト値は0.1 戻り値はArray型で返され、一つ目の要素がモデルの切片、二つ目以降の要素は従属変数xの係数を表します。</p><p>構築したモデルを使った予測は以下の関数で行えます。</p><ul><li>Linear_Regression.RR.predict()</li></ul><pre><code class="nohighlight hljs">- x：従属変数の行列
- w：フィットして得たwのArray</code></pre><p>返り値はArray型で返されます。</p><h3 id="ラッソ回帰の使い方"><a class="docs-heading-anchor" href="#ラッソ回帰の使い方">ラッソ回帰の使い方</a><a id="ラッソ回帰の使い方-1"></a><a class="docs-heading-anchor-permalink" href="#ラッソ回帰の使い方" title="Permalink"></a></h3><p>ラッソ回帰は以下のようにして使うことができます。</p><pre><code class="nohighlight hljs">using LearningHorse
x =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]
t = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]
w = Linear_Regression.LR.fit(x, t)
println(w) # [144.61846209854855, 0.951158307335192, 0.0] 一つ目の要素が切片、二つ目以降が従属変数xの係数を表す</code></pre><p>引数は以下の通りです。</p><ul><li>Linear_Regression.LR.fit()</li></ul><pre><code class="nohighlight hljs">- x：従属変数の行列
- t：目的変数の行列</code></pre><p>以下キーワード引数 	- alpha：学習率、デフォルト値は0.1 	- tol：繰り返しをやめる誤差の絶対値の闘値、デフォルト値は0.0001 	- mi：繰り返しの最大回数、デフォルト値は1000000 戻り値はArray型で返され、一つ目の要素はモデルの切片、二つ目以降が従属変数xの係数を表します。</p><p>構築したモデルを使った予測は以下の関数で行えます。</p><ul><li>Linear_Regression.LR.predict()</li></ul><pre><code class="nohighlight hljs">- x：従属変数
- w：フィットして得られたwのArray</code></pre><p>返り値はLinearAlgebra.Adjoint型で返されます。</p><h3 id="基底関数モデルの使い方"><a class="docs-heading-anchor" href="#基底関数モデルの使い方">基底関数モデルの使い方</a><a id="基底関数モデルの使い方-1"></a><a class="docs-heading-anchor-permalink" href="#基底関数モデルの使い方" title="Permalink"></a></h3><p>基底関数モデルは以下のようにして使うことができます。</p><pre><code class="nohighlight hljs">using LearningHorse
 x =[15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76; 70.43 58.15 37.22 56.51 57.32 40.84 57.79 56.94 63.03 65.69 62.33 64.95 57.73 66.89 46.68 61.08]
t = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]

#ガウス基底関数を使用
w, m, s = LinearRegression.BFM.fit(x[1, :], t, 4)
println(&quot;w:&quot;, w, &quot;m:&quot;, m, &quot;s:&quot;, s)
#w:[-10.6733; 65.4475; -17.7623; 66.6557; 104.918]m:Any[5, 12.3333, 19.6667, 27.0]s:7.333333333333333

p = LinearRegression.BFM.predict(x[1, :], w, m, s)
println(p)
#[165.07, 168.55, 132.278, 162.546, 150.433, 144.04, 154.507, 164.011, 164.902, 165.417, 165.082, 167.712, 156.153, 169.583, 135.749, 167.366]

#多項式基底関数を使用
w, m, s = LinearRegression.BFM.fit(x[1, :], t, 4, alpha = 0.1, f = &quot;polynomial&quot;)
println(&quot;w, m, s(polynomial):&quot;, w, m, s)
#w, m, s(polynomial):[30.074721512969795; 17.649472753936088; -0.9665443355191238; 0.01733639801805678; 30.074721512971067]Any[5, 12.333333333333332, 19.666666666666664, 26.999999999999996]7.333333333333333

p = LinearRegression.BFM.predict(x[1, :], w, m, s, f = &quot;polynomial&quot;)
println(&quot;p(polynomial):&quot;, p)
#p(polynomial):[166.04905485192677, 165.72435351634755, 126.40024815990033, 163.70105505929791, 151.81467689141562, 144.29063632133014, 156.07721189214902, 165.05792844782184, 165.89978336872304, 165.6418324683704, 166.05887282906252, 165.26960817342376,157.70790163159944, 173.13926201582598, 132.39231072417851, 165.16779149696566]</code></pre><p>関数は、predictとfitのみですが、キーワード引数 f を指定することで、多項式回帰とガウス基底関数モデルの二つのモデルを使うことができます。 引数は以下の通りです。</p><p>学習する関数</p><ul><li>LinearRegression.BFM.fit()</li></ul><pre><code class="nohighlight hljs">- x：従属変数のベクトル（行列を入れることはできません）
- t：目的変数のベクトル（行列を入れることはできません）
- M：フィットする次元数、2次元ならM = 2とします。</code></pre><p>以下、キーワード引数 	- m：ガウス基底関数の場合に指定することができます。デフォルトでは各ガウス関数の中心のベクトルを使用します 	- s：mと同様、ガウス関数の場合に指定することができます。デフォルトでは各ガウス関数の間隔を表すスカラーを使用します 	- alpha：計画行列の計算を行う際に加算される正則化項にかけられる係数です。デフォルト値は0.0 	- f：基底関数の種類の指定、ガウス関数は文字列&quot;gauss&quot;、多項式回帰の場合は&quot;polynomial&quot;を渡します。デフォルト値は&quot;gauss&quot;であることに注意してください</p><p>予測する関数</p><ul><li>LinearRegression.BFM.predict()</li></ul><pre><code class="nohighlight hljs">- x：予測に使用する従属変数のベクトル
- w：フィットして得られたモデルのArrayのwを渡す</code></pre><h2 id="分類の関数"><a class="docs-heading-anchor" href="#分類の関数">分類の関数</a><a id="分類の関数-1"></a><a class="docs-heading-anchor-permalink" href="#分類の関数" title="Permalink"></a></h2><h3 id="ラベルエンコーダーの使い方"><a class="docs-heading-anchor" href="#ラベルエンコーダーの使い方">ラベルエンコーダーの使い方</a><a id="ラベルエンコーダーの使い方-1"></a><a class="docs-heading-anchor-permalink" href="#ラベルエンコーダーの使い方" title="Permalink"></a></h3><p>ラベルエンコーダーは以下のように使います。</p><pre><code class="nohighlight hljs">using LearningHorse
label = [&quot;Apple&quot;, &quot;Apple&quot;, &quot;Pear&quot;, &quot;Pear&quot;, &quot;Lemon&quot;, &quot;Apple&quot;, &quot;Pear&quot;, &quot;Lemon&quot;]
l, d = Classification.LE(label)
println(l)
#[3.0 3.0 1.0 1.0 2.0 3.0 1.0 2.0]
println(&quot;d:&quot;, d)
#d:Dict{Any, Any}(&quot;Pear&quot; =&gt; 1, &quot;Lemon&quot; =&gt; 2, &quot;Apple&quot; =&gt; 3)</code></pre><p>ラベルエンコーダーはSet型を使用して実装されているため要素の順番は保証されません、そのため、どのクラスがどのラベルに対応するのかを示すDictが返されます。引数は以下の通りです。</p><ul><li>Classification.LE()</li></ul><pre><code class="nohighlight hljs">- t：変換するラベルのArray</code></pre><h3 id="One-Hot形式への変換"><a class="docs-heading-anchor" href="#One-Hot形式への変換">One Hot形式への変換</a><a id="One-Hot形式への変換-1"></a><a class="docs-heading-anchor-permalink" href="#One-Hot形式への変換" title="Permalink"></a></h3><p>One Hot形式への変換を行うCTOH関数は以下のようにして使います。（ただしこのコードは先ほどのコードの続きとします。）</p><pre><code class="nohighlight hljs">t = Classification.CTOH(l)
println(&quot;t:&quot;, t)
#t:[0.0 0.0 1.0; 0.0 0.0 1.0; 1.0 0.0 0.0; 1.0 0.0 0.0; 0.0 1.0 0.0; 0.0 0.0 1.0; 1.0 0.0 0.0; 0.0 1.0 0.0]</code></pre><p>この関数は渡されるベクトルのクラスの表し方が、0から始まるものは変換可能ですが、クラスを負の数で表したベクトルを渡すとエラーとなリます。 引数は以下の通りです。</p><ul><li>Classification.CTOH()</li></ul><pre><code class="nohighlight hljs">- t：変換するクラスを数値で表したベクトル。</code></pre><h3 id="マルチクラスソフトマックスを使用したマルチクラス分類"><a class="docs-heading-anchor" href="#マルチクラスソフトマックスを使用したマルチクラス分類">マルチクラスソフトマックスを使用したマルチクラス分類</a><a id="マルチクラスソフトマックスを使用したマルチクラス分類-1"></a><a class="docs-heading-anchor-permalink" href="#マルチクラスソフトマックスを使用したマルチクラス分類" title="Permalink"></a></h3><p>MS関数を使用したマルチクラスソフトマックスを使用した分類は以下のように行います。</p><pre><code class="nohighlight hljs">x = [5.1 3.5; 4.9 3.0; 4.7 3.2; 4.6 3.1; 5.0 3.6; 5.4 3.9; 4.6 3.4; 5.0 3.4; 4.4 2.9; 4.9 3.1; 5.4 3.7; 4.8 3.4; 4.8 3.0; 4.3 3.0; 5.8 4.0; 5.7 4.4; 5.4 3.9; 5.1 3.5; 5.7 3.8; 5.1 3.8; 5.4 3.4; 5.1 3.7; 4.6 3.6; 5.1 3.3; 4.8 3.4; 5.0 3.0; 5.0 3.4; 5.2 3.5; 5.2 3.4; 4.7 3.2; 4.8 3.1; 5.4 3.4; 5.2 4.1; 5.5 4.2; 4.9 3.1; 5.0 3.2; 5.5 3.5; 4.9 3.6; 4.4 3.0; 5.1 3.4; 5.0 3.5; 4.5 2.3; 4.4 3.2; 5.0 3.5; 5.1 3.8; 4.8 3.0; 5.1 3.8; 4.6 3.2; 5.3 3.7; 5.0 3.3; 7.0 3.2; 6.4 3.2; 6.9 3.1; 5.5 2.3; 6.5 2.8; 5.7 2.8; 6.3 3.3; 4.9 2.4; 6.6 2.9; 5.2 2.7; 5.0 2.0; 5.9 3.0; 6.0 2.2; 6.1 2.9; 5.6 2.9; 6.7 3.1; 5.6 3.0; 5.8 2.7; 6.2 2.2; 5.6 2.5; 5.9 3.2; 6.1 2.8; 6.3 2.5; 6.1 2.8; 6.4 2.9; 6.6 3.0; 6.8 2.8; 6.7 3.0; 6.0 2.9; 5.7 2.6; 5.5 2.4; 5.5 2.4; 5.8 2.7; 6.0 2.7; 5.4 3.0; 6.0 3.4; 6.7 3.1; 6.3 2.3; 5.6 3.0; 5.5 2.5; 5.5 2.6; 6.1 3.0; 5.8 2.6; 5.0 2.3; 5.6 2.7; 5.7 3.0; 5.7 2.9; 6.2 2.9; 5.1 2.5; 5.7 2.8; 6.3 3.3; 5.8 2.7; 7.1 3.0; 6.3 2.9; 6.5 3.0; 7.6 3.0; 4.9 2.5; 7.3 2.9; 6.7 2.5; 7.2 3.6; 6.5 3.2; 6.4 2.7; 6.8 3.0; 5.7 2.5; 5.8 2.8; 6.4 3.2; 6.5 3.0; 7.7 3.8; 7.7 2.6; 6.0 2.2; 6.9 3.2; 5.6 2.8; 7.7 2.8; 6.3 2.7; 6.7 3.3; 7.2 3.2; 6.2 2.8; 6.1 3.0; 6.4 2.8; 7.2 3.0; 7.4 2.8; 7.9 3.8; 6.4 2.8; 6.3 2.8; 6.1 2.6; 7.7 3.0; 6.3 3.4; 6.4 3.1; 6.0 3.0; 6.9 3.1; 6.7 3.1; 6.9 3.1; 5.8 2.7; 6.8 3.2; 6.7 3.3; 6.7 3.0; 6.3 2.5; 6.5 3.0; 6.2 3.4; 5.9 3.0]
t = [0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2]
t = Classification.CTOH(t)
w = Classification.MS.fit(x, t, alpha = 0.1)
println(w)
#[1.7209083498295872 1.8953943405297293 -0.6163026903593205; -1.256172465412238 1.8287641755675748 2.4274082898446574; 4.812985176776389 -0.7076002068908029 -1.1053849698855869]
p = Classification.MS.predict(x, w)
println(p)
#[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 1, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2]</code></pre><p>引数は以下の通りです。</p><p>学習を行う関数</p><ul><li>Classification.MS.fit()</li></ul><pre><code class="nohighlight hljs">- x：従属変数の行列（Array）
- t：LEやCTOHを使用して整形したクラスの目的変数の行列（Array）</code></pre><p>以下はキーワード引数 	- alpha：最適なパラメータを勾配最急下法で探索する際の学習率、デフォルト値は0.01 	- tau_max：alphaと同じくパラメータを最適化する際の最大繰り返し回数、デフォルト値は1000</p><p>予測を行う関数</p><ul><li>Classification.MS.predict()</li></ul><pre><code class="nohighlight hljs">- x：予測に使用する従属変数の行列
- w：フィットして得られたパラメータwのArray</code></pre><h3 id="One-vs-Restを使用したマルチクラス分類"><a class="docs-heading-anchor" href="#One-vs-Restを使用したマルチクラス分類">One-vs-Restを使用したマルチクラス分類</a><a id="One-vs-Restを使用したマルチクラス分類-1"></a><a class="docs-heading-anchor-permalink" href="#One-vs-Restを使用したマルチクラス分類" title="Permalink"></a></h3><p>One-vs-Restは以下のようにして使用することができます。（このコードは先ほどのマルチクラスソフトマックスの続きとします。）</p><pre><code class="nohighlight hljs">w = Classification.OVR.fit(x, t; alpha = 0.1)
println(&quot;w:&quot;, w)
#w:Any[[1.4720211645121106 -3.049021804351106 4.768579857703322], [1.496138559080458 0.5690372319250491 -1.8729783593510938], [-1.2013066350855492 1.258936673691226 -2.3333697507254283]]
p = Classification.OVR.predict(x, w)
println(&quot;predict:&quot;, p)
#predict:Any[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 1, 1, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3,3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2]</code></pre><p>引数は以下の通りです。</p><p>学習を行う関数</p><ul><li>Classification.OVR.fit()</li></ul><pre><code class="nohighlight hljs">- x：従属変数の行列（Array）
- t：LEやCTOHを使用して整形したクラスの目的変数の行列（Array）</code></pre><p>以下はキーワード引数 	- alpha：最適なパラメータを勾配最急下法で探索する際の学習率、デフォルト値は0.01 	- tau_max：alphaと同じくパラメータを最適化する際の最大繰り返し回数、デフォルト値は1000</p><p>予測を行う関数</p><ul><li>Classification.OVR.predict()</li></ul><pre><code class="nohighlight hljs">- x：予測に使用する従属変数の行列
- w：fitで得られたパラメータwのArray</code></pre><h2 id="損失関数"><a class="docs-heading-anchor" href="#損失関数">損失関数</a><a id="損失関数-1"></a><a class="docs-heading-anchor-permalink" href="#損失関数" title="Permalink"></a></h2><h3 id="MSE（平均二乗誤差）の使い方"><a class="docs-heading-anchor" href="#MSE（平均二乗誤差）の使い方">MSE（平均二乗誤差）の使い方</a><a id="MSE（平均二乗誤差）の使い方-1"></a><a class="docs-heading-anchor-permalink" href="#MSE（平均二乗誤差）の使い方" title="Permalink"></a></h3><p>平均二乗誤差は以下のようにして使うことができます。</p><pre><code class="nohighlight hljs">using LearningHorse
mean = Loss_Function.MSE([1, 2, 3, 4], [1, 2, 3, 4], [1, 2])
println(mean) #4

x = [15.43 23.01 5.0 12.56 8.67 7.31 9.66 13.64 14.92 18.47 15.48 22.13 10.11 26.95 5.68 21.76]
t = [170.91 160.68 129.0 159.7 155.46 140.56 153.65 159.43 164.7 169.65 160.71 173.29 159.31 171.52 138.96 165.87]

#ガウス基底関数の平均二乗誤差
w, m, s = LinearRegression.BFM.fit(x[1, :], t, 4)
mse = LossFunction.MSE(x[1, :], t, w, b = &quot;gauss&quot;, m = m, s = s)
println(&quot;mse(gauss):&quot;, mse)
#mse(gauss):16.777305554499097

#多項式回帰の平均二乗誤差
w, m, s = LinearRegression.BFM.fit(x[1, :], t, 4, alpha = 0.1, f = &quot;polynomial&quot;)
mse = LossFunction.MSE(x[1, :], t, w, b = &quot;polynomial&quot;, m = m, s = s)
println(&quot;mse(polynomial):&quot;, mse)
#mse(polynomial):18.491924125591847</code></pre><p>引数bを指定することによって、基底関数モデルのMSEの算出が可能です。 引数は以下の通りです。</p><ul><li>LossFunction.MSE()</li></ul><pre><code class="nohighlight hljs">- x：従属変数の行列
- t：目的変数の行列
- w：フィットして得られたモデルのパラメータ</code></pre><p>以下キーワード引数 	- b：基底関数のMSEを算出する際に指定する。ガウス基底関数なら&quot;gauss&quot;、多項式回帰なら&quot;polynomial&quot; 	- m：ガウス基底関数のMSEを算出する際に指定できる。デフォルトでは各ガウス関数の中心のベクトルを使用して計算 	- s：ガウス関数のMSEを算出する際に指定できる。デフォルトでは各ガウス関数の間隔を使用して計算 	- mean_f：二乗して足し合わせた誤差の総和を割って、平均二乗誤差にするかどうか、デフォルト値はtrue</p><h3 id="CEE（交差エントロピー誤差）の使い方"><a class="docs-heading-anchor" href="#CEE（交差エントロピー誤差）の使い方">CEE（交差エントロピー誤差）の使い方</a><a id="CEE（交差エントロピー誤差）の使い方-1"></a><a class="docs-heading-anchor-permalink" href="#CEE（交差エントロピー誤差）の使い方" title="Permalink"></a></h3><p>交差エントロピー誤差は以下のようにして使います。</p><pre><code class="nohighlight hljs">using LearningHorse
#MSのCEE
x = [5.1 3.5; 4.9 3.0; 4.7 3.2; 4.6 3.1; 5.0 3.6; 5.4 3.9; 4.6 3.4; 5.0 3.4; 4.4 2.9; 4.9 3.1; 5.4 3.7; 4.8 3.4; 4.8 3.0; 4.3 3.0; 5.8 4.0; 5.7 4.4; 5.4 3.9; 5.1 3.5; 5.7 3.8; 5.1 3.8; 5.4 3.4; 5.1 3.7; 4.6 3.6; 5.1 3.3; 4.8 3.4; 5.0 3.0; 5.0 3.4; 5.2 3.5; 5.2 3.4; 4.7 3.2; 4.8 3.1; 5.4 3.4; 5.2 4.1; 5.5 4.2; 4.9 3.1; 5.0 3.2; 5.5 3.5; 4.9 3.6; 4.4 3.0; 5.1 3.4; 5.0 3.5; 4.5 2.3; 4.4 3.2; 5.0 3.5; 5.1 3.8; 4.8 3.0; 5.1 3.8; 4.6 3.2; 5.3 3.7; 5.0 3.3; 7.0 3.2; 6.4 3.2; 6.9 3.1; 5.5 2.3; 6.5 2.8; 5.7 2.8; 6.3 3.3; 4.9 2.4; 6.6 2.9; 5.2 2.7; 5.0 2.0; 5.9 3.0; 6.0 2.2; 6.1 2.9; 5.6 2.9; 6.7 3.1; 5.6 3.0; 5.8 2.7; 6.2 2.2; 5.6 2.5; 5.9 3.2; 6.1 2.8; 6.3 2.5; 6.1 2.8; 6.4 2.9; 6.6 3.0; 6.8 2.8; 6.7 3.0; 6.0 2.9; 5.7 2.6; 5.5 2.4; 5.5 2.4; 5.8 2.7; 6.0 2.7; 5.4 3.0; 6.0 3.4; 6.7 3.1; 6.3 2.3; 5.6 3.0; 5.5 2.5; 5.5 2.6; 6.1 3.0; 5.8 2.6; 5.0 2.3; 5.6 2.7; 5.7 3.0; 5.7 2.9; 6.2 2.9; 5.1 2.5; 5.7 2.8; 6.3 3.3; 5.8 2.7; 7.1 3.0; 6.3 2.9; 6.5 3.0; 7.6 3.0; 4.9 2.5; 7.3 2.9; 6.7 2.5; 7.2 3.6; 6.5 3.2; 6.4 2.7; 6.8 3.0; 5.7 2.5; 5.8 2.8; 6.4 3.2; 6.5 3.0; 7.7 3.8; 7.7 2.6; 6.0 2.2; 6.9 3.2; 5.6 2.8; 7.7 2.8; 6.3 2.7; 6.7 3.3; 7.2 3.2; 6.2 2.8; 6.1 3.0; 6.4 2.8; 7.2 3.0; 7.4 2.8; 7.9 3.8; 6.4 2.8; 6.3 2.8; 6.1 2.6; 7.7 3.0; 6.3 3.4; 6.4 3.1; 6.0 3.0; 6.9 3.1; 6.7 3.1; 6.9 3.1; 5.8 2.7; 6.8 3.2; 6.7 3.3; 6.7 3.0; 6.3 2.5; 6.5 3.0; 6.2 3.4; 5.9 3.0]
t = [0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2]
t = Classification.CTOH(t)
w = Classification.MS.fit(x, t, alpha = 0.1)
cee = LossFunction.CEE(x, t, w, t_f = true)
println(&quot;cee:&quot;, cee)
#52.20173706758349

#OVRのCEE
w = Classification.OVR.fit(x, t; alpha = 0.1)
cee = LossFunction.CEE(x, t, w, sigmoid_f = true)
println(&quot;cee:&quot;, cee)
#28.702950289767305</code></pre><p>引数は以下の通りです。</p><ul><li>LossFunction.CEE()</li></ul><pre><code class="nohighlight hljs">- x：従属変数の行列
- t：目的変数の行列
- w：フィットして得られたモデルのパラメータ</code></pre><p>以下キーワード引数 	- mean<em>f：誤差の和を割って平均交差エントロピー誤差にするかどうか、デフォルトではtrue 	- sigmoid</em>f：OVRでCEEを求める場合は、この引数をtrueにする必要があります。デフォルト値はfalse 	- t_f：CEEの求める式を指定します。trueなら（式−1）に、falseなら（式-2）になります。デフォルト値はfalse</p><p>（Nは要素の数、Kは入力の次元数） （式-1）<span>$-\,\frac{1}{N}\,\sum_{n=0}^{N-1} {t_n\,\log\,y_n\,+\,(1\,-\,t_n)\log(1\,-\,y_n)}$</span></p><p>（式-2）<span>$-\,\frac{1}{N}\,\sum_{n=0}^{N-1}\sum_{k=0}^{K-1}\,t_{nk}\,\log\,y_{nk}$</span></p><h2 id="Preprocessing"><a class="docs-heading-anchor" href="#Preprocessing">Preprocessing</a><a id="Preprocessing-1"></a><a class="docs-heading-anchor-permalink" href="#Preprocessing" title="Permalink"></a></h2><p>Preprocessingの関数は、全て引数や仕様が同じなので、Standard Scalerのみ書きます。</p><h3 id="Standard-Scalerの使い方"><a class="docs-heading-anchor" href="#Standard-Scalerの使い方">Standard Scalerの使い方</a><a id="Standard-Scalerの使い方-1"></a><a class="docs-heading-anchor-permalink" href="#Standard-Scalerの使い方" title="Permalink"></a></h3><p>StandardScaler（及びその他のPreprocessingの関数）は以下のように使います。</p><pre><code class="nohighlight hljs">x = [1 2 3 4 5; 2 3 4 5 6]]
sd = Preprocessing.SS.fit_transform(x, axis = 2)
println(&quot;sd:&quot;, sd)
#sd:(Any[[-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518], [-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518]], Any[[3.0, 1.5811388300841898], [4.0, 1.5811388300841898]])
inv = Preprocessing.SS.inverse_transform(sd[1], sd[2], axis = 2)
println(&quot;inv:&quot;, inv)
#inv:Any[[1.0 2.0 3.0 4.0 5.0], [2.0 3.0 4.0 5.0 6.0]]
p = Preprocessing.SS.fit(x, axis = 2)
println(&quot;fit:&quot;, p)
#fit:Any[[3.0, 1.5811388300841898], [4.0, 1.5811388300841898]]
p = Preprocessing.SS.transform(x, p, axis = 2)
println(&quot;p:&quot;, p)
#p:Any[[-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518], [-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518]]</code></pre><p>fit関数を使ったのち、transform関数を使うのと、fit<em>transform関数を使うのは完全に等価です。 fit関数は、スケーリングに必要な数値を算出し返すだけの関数で、fitで得られた数値を使ってスケーリングするのがtransform関数です。fit</em>transform関数はこれらの処理を一度に行います。 一方、invers_transform関数は、スケーリングした数値やモデルで予測した値を元の値に戻す関数です。 引数は以下の通りです。</p><ul><li>Preprocessing.SS.fit_transform()</li></ul><pre><code class="nohighlight hljs">- x：スケーリングする行列
- axis：スケーリングする軸、各特徴量が各列に並んでいればaxis = 1、各行に並んでいればaxis = 2を指定する。デフォルト値は1</code></pre><ul><li>Preprocessing.SS.fit()</li></ul><pre><code class="nohighlight hljs">- x：スケーリングする行列
-axis：スケーリングする軸、各特徴量が各列に並んでいればaxis = 1、各行に並んでいればaxis = 2を指定する。デフォルト値は1</code></pre><ul><li>Preprocessing.SS.transform()</li></ul><pre><code class="nohighlight hljs">- x：スケーリングする行列
- p：fit関数やfit_transform関数で得られた数値pを渡す
- axis：スケーリングする軸、各特徴量が各列に並んでいればaxis = 1、各行に並んでいればaxis = 2を指定する。デフォルト値は1</code></pre><ul><li>Preprocessing.SS.invers_transform()</li></ul><pre><code class="nohighlight hljs">- x：逆にスケーリングする行列
- p：スケーリングする際に求められた数値pを渡す
- axis：逆にスケーリングする軸、各特徴量が各列に並んでいればaxis = 1、各行に並んでいればaxis = 2を指定する。デフォルト値は1</code></pre><h3 id="その他の関数"><a class="docs-heading-anchor" href="#その他の関数">その他の関数</a><a id="その他の関数-1"></a><a class="docs-heading-anchor-permalink" href="#その他の関数" title="Permalink"></a></h3><p>MinMaxScalerはPreprocessing.MMで使うことができます。 MMは以下のような式に基づいてスケーリングします。 <span>$\tilde{\boldsymbol{x}} = \frac{\boldsymbol{x}-min(\boldsymbol{x})}{max(\boldsymbol{x})-min(\boldsymbol{x})}$</span></p><p>RobustScalerはPreprocessing.RSで使うことができます。 RSは以下のような式に基づいてスケーリングします。 <span>$\tilde{\boldsymbol{x}} = \frac{\boldsymbol{x}-Q2}{Q3 - Q1}$</span></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../NeuralNetwork/">« NeuralNetwork</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Wednesday 1 December 2021 19:42">Wednesday 1 December 2021</span>. Using Julia version 1.6.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
